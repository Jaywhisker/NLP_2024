{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for exploring finetuning using Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "import csv\n",
    "from torch.optim import AdamW\n",
    "from collections import Counter\n",
    "\n",
    "import logging\n",
    "# set the logging level to INFO\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from flair.datasets import ClassificationCorpus\n",
    "from flair.embeddings import (\n",
    "    FlairEmbeddings,\n",
    "    DocumentRNNEmbeddings,\n",
    "    TransformerWordEmbeddings,\n",
    "    TransformerDocumentEmbeddings\n",
    ")\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "import flair\n",
    "flair.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {flair.device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    '''Data processor to ensure datasets adhere to Flair-compatible format.'''\n",
    "    \n",
    "    def __init__(self, output_dir: str):\n",
    "        \"\"\"\n",
    "        initializes the DataProcessor class with the output directory.\n",
    "        args:\n",
    "            output_dir (str): Directory to save processed files.\n",
    "        \"\"\"\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def read_imdb(self, data_dir: str, is_train: bool) -> list:\n",
    "        \"\"\"\n",
    "        reads IMDb dataset and returns it in Flair-compatible format.\n",
    "        args:\n",
    "            data_dir (str): Path to the IMDb dataset directory.\n",
    "            is_train (bool): If True, process training data; else, process test data.\n",
    "        returns:\n",
    "            list: List of sentences with labels in Flair format.\n",
    "        \"\"\"\n",
    "        data_folder = 'train' if is_train else 'test'\n",
    "        data = []\n",
    "\n",
    "        for label_folder in ['neg', 'pos']:\n",
    "            path = os.path.join(data_dir, data_folder, label_folder)\n",
    "            label = '__label__NEG' if label_folder == 'neg' else '__label__POS'\n",
    "\n",
    "            for file in os.listdir(path):\n",
    "                with open(os.path.join(path, file), 'r', encoding='utf-8') as f:\n",
    "                    text = f.read().replace('\\n', ' ')\n",
    "                    data.append(f\"{label} {text}\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    def read_csv(self, csv_path: str, label_column_index: int, text_column_index: int) -> list:\n",
    "        \"\"\"\n",
    "        reads a CSV file and returns data in Flair-compatible format.\n",
    "        args:\n",
    "            csv_path (str): Path to the CSV file.\n",
    "            label_column_index (int): Index of the label column.\n",
    "            text_column_index (int): Index of the text column.\n",
    "        returns:\n",
    "            list: List of sentences with labels in Flair format.\n",
    "        \"\"\"\n",
    "        # Read CSV while skipping the header row\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, header=0, encoding=\"utf-8\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error reading CSV file: {e}\")\n",
    "\n",
    "        # Map labels and clean text\n",
    "        label_mapping = {0: \"NEG\", 1: \"POS\"}\n",
    "        data = []\n",
    "        for index, row in df.iterrows():\n",
    "            try:\n",
    "                # Map label and clean text\n",
    "                label = label_mapping[int(row[label_column_index])]\n",
    "                text = str(row[text_column_index]).replace(\"\\n\", \" \")\n",
    "                data.append(f\"__label__{label} {text}\")\n",
    "            except (KeyError, ValueError):\n",
    "                raise ValueError(\n",
    "                    f\"Invalid data found in row: {row}. Expected label values: {list(label_mapping.keys())}.\"\n",
    "                )\n",
    "        return data\n",
    "\n",
    "\n",
    "    def save_data(self, data_list: list, file_name: str):\n",
    "        \"\"\"\n",
    "        saves a list of sentences to a file in the output directory.\n",
    "        args:\n",
    "            data_list (list): List of sentences to save.\n",
    "            file_name (str): Name of the output file.\n",
    "        \"\"\"\n",
    "        output_path = os.path.join(self.output_dir, file_name)\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for item in data_list:\n",
    "                f.write(f\"{item}\\n\")\n",
    "\n",
    "        print(f\"Saved data to {output_path}\")\n",
    "\n",
    "    def split_data(self, data: list, dev_size: float = 0.1, random_state: int = 42) -> tuple:\n",
    "        \"\"\"\n",
    "        splits data into training and validation sets.\n",
    "        args:\n",
    "            data (list): Full dataset.\n",
    "            dev_size (float): Proportion of data to use for validation.\n",
    "            random_state (int): Random seed for reproducibility.\n",
    "        returns:\n",
    "            tuple: Training and validation sets.\n",
    "        \"\"\"\n",
    "        train_data, dev_data = train_test_split(data, test_size=dev_size, random_state=random_state)\n",
    "        return train_data, dev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory ensured and flair path set to: ../data/flair_data\n"
     ]
    }
   ],
   "source": [
    "# initialize DataProcessor\n",
    "data_dir = \"../data/aclImdb\"\n",
    "\n",
    "try:\n",
    "    # ensure output directory exists\n",
    "    flair_data_folder = \"../data/flair_data\"\n",
    "    os.makedirs(flair_data_folder, exist_ok=True)\n",
    "    \n",
    "    print(f\"Directory ensured and flair path set to: {flair_data_folder}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while setting up the Flair data folder: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to ../data/flair_data/train.txt\n",
      "Saved data to ../data/flair_data/dev.txt\n",
      "Saved data to ../data/flair_data/test.txt\n"
     ]
    }
   ],
   "source": [
    "processor = DataProcessor(output_dir=flair_data_folder)\n",
    "\n",
    "# process IMDb dataset\n",
    "train_data = processor.read_imdb(data_dir, is_train=True)\n",
    "test_data = processor.read_imdb(data_dir, is_train=False)\n",
    "\n",
    "# split training data into training and validation sets\n",
    "train_data, dev_data = processor.split_data(train_data, dev_size=0.1)\n",
    "\n",
    "# save the datasets\n",
    "processor.save_data(train_data, 'train.txt')\n",
    "processor.save_data(dev_data, 'dev.txt')\n",
    "processor.save_data(test_data, 'test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating corpus for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-11 21:18:33,691 Reading data from ../data/flair_data\n",
      "2024-12-11 21:18:33,693 Train: ../data/flair_data/train.txt\n",
      "2024-12-11 21:18:33,693 Dev: ../data/flair_data/dev.txt\n",
      "2024-12-11 21:18:33,694 Test: ../data/flair_data/test.txt\n",
      "2024-12-11 21:18:34,189 Initialized corpus ../data/flair_data (label type name is 'sentiment')\n",
      "Number of training sentences: 22500\n",
      "Number of validation sentences: 2500\n",
      "Number of test sentences: 25000\n"
     ]
    }
   ],
   "source": [
    "corpus_folder = Path(flair_data_folder)\n",
    "\n",
    "# create training corpus\n",
    "training_corpus = ClassificationCorpus(\n",
    "    corpus_folder,\n",
    "    train_file='train.txt',\n",
    "    dev_file='dev.txt',\n",
    "    test_file='test.txt',\n",
    "    label_type='sentiment'\n",
    ")\n",
    "\n",
    "# print statistics\n",
    "print(f\"Number of training sentences: {len(training_corpus.train)}\")\n",
    "print(f\"Number of validation sentences: {len(training_corpus.dev)}\")\n",
    "print(f\"Number of test sentences: {len(training_corpus.test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create label dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-11 21:19:43,757 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "22500it [00:39, 571.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-11 21:20:23,183 Dictionary created for label 'sentiment' with 2 values: POS (seen 11298 times), NEG (seen 11202 times)\n",
      "Dictionary with 2 tags: POS, NEG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "label_dict = training_corpus.make_label_dictionary(label_type='sentiment')\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flair’s ModelTrainer not only logs the training process with evaluation against the dev.txt dataset at each epoch but also identifies the best-performing model based on the F1 score. At the end of all epochs, this best model is automatically evaluated against the test.txt dataset specified in the ClassificationCorpus, providing validation for the training results.\n",
    "\n",
    "However, the evaluate_model function was created to allow evaluation of the best model against any input dataset, especially in scenarios where training is terminated early due to extended runtime or observed performance plateau. Additionally, this function was employed to evaluate the test_data_movie.csv dataset provided later on, ensuring flexibility in validating the model on unseen or supplementary datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Embeddings: Flair and Distilbert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise different embeddings\n",
    "flair_forward_embedding = FlairEmbeddings('news-forward')\n",
    "flair_backward_embedding = FlairEmbeddings('news-backward')\n",
    "distilbert_transformer_word_embeddings = TransformerWordEmbeddings('distilbert-base-uncased')\n",
    "\n",
    "# list of embeddings\n",
    "flair_distilbert_embeddings = [\n",
    "    flair_forward_embedding,\n",
    "    flair_backward_embedding,\n",
    "    distilbert_transformer_word_embeddings,\n",
    "]\n",
    "\n",
    "# create document embeddings from stacked embeddings\n",
    "flair_distilbert_document_embeddings = DocumentRNNEmbeddings(\n",
    "    embeddings=flair_distilbert_embeddings,\n",
    "    hidden_size=256,\n",
    "    reproject_words=True,\n",
    "    reproject_words_dimension=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise text classifier model\n",
    "flair_distilbert_classifier = TextClassifier(\n",
    "    flair_distilbert_document_embeddings,\n",
    "    label_dictionary=label_dict,\n",
    "    label_type='sentiment'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine embeddings_storage_mode based on GPU availability\n",
    "embeddings_storage_mode = 'gpu' if torch.cuda.is_available() else 'none'\n",
    "\n",
    "# initialise the trainer\n",
    "flair_distilbert_trainer = ModelTrainer(flair_distilbert_classifier, training_corpus)\n",
    "\n",
    "# fine-tune the model\n",
    "flair_distilbert_trainer.fine_tune(\n",
    "    base_path='../flair_models/flair_distilbert_model',     # directory to save the model and logs\n",
    "    learning_rate=5e-5,                                     \n",
    "    mini_batch_size=16,                                     \n",
    "    max_epochs=10,                                         \n",
    "    embeddings_storage_mode=embeddings_storage_mode,     \n",
    "    optimizer=AdamW,                                        \n",
    "    save_final_model=True,                                  # save the final model\n",
    "    save_model_each_k_epochs=1,                             # save model checkpoint every epoch\n",
    "    create_file_logs=True,                                  # save logs to a file\n",
    "    create_loss_file=True,                                  # save loss values to a file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Embeddings: Flair and RoBERTa-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise different embeddings\n",
    "flair_forward_embedding = FlairEmbeddings('news-forward')\n",
    "flair_backward_embedding = FlairEmbeddings('news-backward')\n",
    "roberta_transformer_word_embeddings = TransformerWordEmbeddings('roberta-large')\n",
    "\n",
    "# list of embeddings\n",
    "flair_roberta_embeddings = [\n",
    "    flair_forward_embedding,\n",
    "    flair_backward_embedding,\n",
    "    roberta_transformer_word_embeddings,\n",
    "]\n",
    "\n",
    "# create document embeddings from stacked embeddings\n",
    "flair_roberta_document_embeddings = DocumentRNNEmbeddings(\n",
    "    embeddings=flair_roberta_embeddings,\n",
    "    hidden_size=256,\n",
    "    reproject_words=True,\n",
    "    reproject_words_dimension=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise text classifier model\n",
    "flair_roberta_classifier = TextClassifier(\n",
    "    flair_roberta_document_embeddings,\n",
    "    label_dictionary=label_dict,\n",
    "    label_type='sentiment'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine embeddings_storage_mode based on GPU availability\n",
    "embeddings_storage_mode = 'gpu' if torch.cuda.is_available() else 'none'\n",
    "\n",
    "# initialise the trainer\n",
    "flair_roberta_trainer = ModelTrainer(flair_roberta_classifier, training_corpus)\n",
    "\n",
    "# fine-tune the model\n",
    "flair_roberta_trainer.fine_tune(\n",
    "    base_path='../flair_models/flair_roberta_model',        # directory to save the model and logs\n",
    "    learning_rate=5e-5,                                     \n",
    "    mini_batch_size=2,                                      \n",
    "    max_epochs=5,                                           \n",
    "    embeddings_storage_mode=embeddings_storage_mode,     \n",
    "    optimizer=AdamW,                                        \n",
    "    save_final_model=True,                                  # save the final model\n",
    "    save_model_each_k_epochs=1,                             # save model checkpoint every epoch\n",
    "    create_file_logs=True,                                  # save logs to a file\n",
    "    create_loss_file=True,                                  # save loss values to a file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flair embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise different embeddings\n",
    "flair_forward_embedding = FlairEmbeddings(\"news-forward\")\n",
    "flair_backward_embedding = FlairEmbeddings(\"news-backward\")\n",
    "\n",
    "# list of embeddings\n",
    "flair_embeddings = [\n",
    "    flair_forward_embedding,\n",
    "    flair_backward_embedding,\n",
    "]\n",
    "\n",
    "# create document embeddings from stacked embeddings\n",
    "flair_document_embeddings = DocumentRNNEmbeddings(\n",
    "    embeddings=flair_embeddings,\n",
    "    hidden_size=256,\n",
    "    reproject_words=True,\n",
    "    reproject_words_dimension=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise text classifier model\n",
    "flair_classifier = TextClassifier(\n",
    "    embeddings=flair_document_embeddings,\n",
    "    label_dictionary=label_dict,\n",
    "    label_type=\"sentiment\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine embeddings_storage_mode based on GPU availability\n",
    "embeddings_storage_mode = 'gpu' if torch.cuda.is_available() else 'none'\n",
    "\n",
    "# initialise the trainer\n",
    "flair_trainer = ModelTrainer(flair_classifier, training_corpus)\n",
    "\n",
    "# fine-tune the model\n",
    "flair_trainer.fine_tune(\n",
    "    base_path='../flair_models/flair_model',        # directory to save the model and logs\n",
    "    learning_rate=5e-5,                             \n",
    "    mini_batch_size=8,                              \n",
    "    max_epochs=5,                                   \n",
    "    embeddings_storage_mode=embeddings_storage_mode,     \n",
    "    optimizer=AdamW,                              \n",
    "    save_final_model=True,                          # save the final model\n",
    "    save_model_each_k_epochs=1,                     # save model checkpoint every epoch\n",
    "    create_file_logs=True,                          # save logs to a file\n",
    "    create_loss_file=True,                          # save loss values to a file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further exploration: RoBERTa\n",
    "\n",
    "While our team initially explored RoBERTa fine-tuning without incorporating Flair, we aim to further experiment and compare the results of fine-tuning RoBERTa both with and without the Flair framework. This comparison seeks to evaluate the impact of Flair’s architecture, including its ability to leverage additional embeddings and optional RNN layers, on the overall performance and computational efficiency of RoBERTa-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize transformer embeddings\n",
    "roberta_document_transformer_embedding = TransformerDocumentEmbeddings(\n",
    "    model=\"roberta-base\",  \n",
    "    fine_tune=True,       \n",
    "    layers=\"-1\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update classifier with transformer embedding\n",
    "roberta_document_transformer_classifier = TextClassifier(\n",
    "    embeddings=roberta_document_transformer_embedding, \n",
    "    label_dictionary=label_dict, \n",
    "    label_type=\"sentiment\").to(flair.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine embeddings_storage_mode based on GPU availability\n",
    "embeddings_storage_mode = 'gpu' if torch.cuda.is_available() else 'none'\n",
    "\n",
    "# initialise the trainer\n",
    "roberta_document_transformer_trainer = ModelTrainer(roberta_document_transformer_classifier, training_corpus)\n",
    "\n",
    "# fine-tune the model\n",
    "roberta_document_transformer_trainer.fine_tune(\n",
    "    base_path='../flair_models/roberta_model',      # directory to save the model and logs\n",
    "    learning_rate=5e-5,                             \n",
    "    mini_batch_size=8,                              \n",
    "    max_epochs=10,                                  \n",
    "    embeddings_storage_mode=embeddings_storage_mode,     \n",
    "    optimizer=AdamW,                                \n",
    "    save_final_model=True,                          # save the final model\n",
    "    save_model_each_k_epochs=1,                     # save model checkpoint every epoch\n",
    "    create_file_logs=True,                          # save logs to a file\n",
    "    create_loss_file=True,                          # save loss values to a file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(classifier, test_dataset, output_csv_path:str=\"../src/outputs/predictions.csv\") -> dict:\n",
    "    \"\"\"\n",
    "    evaluate a Flair classifier on a given test dataset, with verification and debugging steps.\n",
    "    \n",
    "    args:\n",
    "        classifier (TextClassifier): The trained Flair classifier.\n",
    "        test_dataset (Dataset): The test dataset.\n",
    "        output_csv_path (str, optional): Path to save the CSV file containing true and predicted labels for each sentence.\n",
    "    \n",
    "    returns:\n",
    "        dict: Dictionary containing evaluation metrics namely accuracy, precision, recall, and F1 score.\n",
    "    \"\"\"\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    sentences = []\n",
    "\n",
    "    # check device compatibility\n",
    "    classifier.to(flair.device)\n",
    "    test_dataset.to(flair.device)\n",
    "    \n",
    "    # label mapping\n",
    "    label_mapping = {\"NEG\": 0, \"POS\": 1}\n",
    "\n",
    "    # iterate over the test dataset with tqdm progress bar\n",
    "    for sentence in tqdm.tqdm(test_dataset, desc=\"Evaluating\", leave=True):\n",
    "        sentences.append(sentence.to_plain_string())\n",
    "\n",
    "        # true label\n",
    "        true_label = sentence.get_label(\"sentiment\").value\n",
    "        if true_label not in label_mapping:\n",
    "            print(f\"Skipping sentence with unexpected true label: {true_label}\")\n",
    "            continue\n",
    "        true_labels.append(true_label)\n",
    "\n",
    "        # predicted label\n",
    "        classifier.predict(sentence)\n",
    "        predicted_label = sentence.labels[0].value\n",
    "        if predicted_label not in label_mapping:\n",
    "            print(f\"Skipping sentence with unexpected predicted label: {predicted_label}\")\n",
    "            continue\n",
    "        predicted_labels.append(predicted_label)\n",
    "\n",
    "    # map labels to numeric values for sklearn\n",
    "    try:\n",
    "        true_labels_mapped = [label_mapping[label] for label in true_labels]\n",
    "        predicted_labels_mapped = [label_mapping[label] for label in predicted_labels]\n",
    "    except KeyError as e:\n",
    "        print(f\"Label mapping error: {e}. Ensure all labels are in {label_mapping}.\")\n",
    "        return {}\n",
    "\n",
    "    # write predictions to a CSV file\n",
    "    with open(output_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Sentence\", \"Actual\", \"Predicted\"])  # Header\n",
    "        for sentence, true_label, predicted_label in zip(sentences, true_labels, predicted_labels):\n",
    "            writer.writerow([sentence, true_label, predicted_label])\n",
    "    print(f\"Predictions saved to {output_csv_path}\")\n",
    "\n",
    "    accuracy = accuracy_score(true_labels_mapped, predicted_labels_mapped)\n",
    "    precision = precision_score(true_labels_mapped, predicted_labels_mapped, pos_label=1, zero_division=0)\n",
    "    recall = recall_score(true_labels_mapped, predicted_labels_mapped, pos_label=1, zero_division=0)\n",
    "    f1 = f1_score(true_labels_mapped, predicted_labels_mapped, pos_label=1, zero_division=0)\n",
    "\n",
    "    classification_rep = classification_report(\n",
    "        true_labels_mapped,\n",
    "        predicted_labels_mapped,\n",
    "        target_names=[\"NEG\", \"POS\"]\n",
    "    )\n",
    "\n",
    "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_rep)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"classification_report\": classification_rep,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Evaluation on test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the best model\n",
    "flair_distilbert_model_path = \"../flair_models/flair_distilbert_model/best-model.pt\"\n",
    "\n",
    "# load the trained model\n",
    "trained_flair_distilbert_classifier = TextClassifier.load(flair_distilbert_model_path)\n",
    "\n",
    "# evaluate the model\n",
    "flair_distilbert_results = evaluate_model(trained_flair_distilbert_classifier, training_corpus.test, output_csv_path=\"../src/outputs/flair_distilbert_model_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the best model\n",
    "flair_roberta_model_path = \"../flair_models/flair_roberta_model/best-model.pt\"\n",
    "\n",
    "# load the trained model\n",
    "trained_flair_roberta_classifier = TextClassifier.load(flair_roberta_model_path)\n",
    "\n",
    "# evaluate the model\n",
    "flair_roberta_results = evaluate_model(trained_flair_roberta_classifier, training_corpus.test, output_csv_path=\"../src/outputs/flair_roberta_model_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the best model\n",
    "flair_model_path = \"../flair_models/flair_model/best-model.pt\"\n",
    "\n",
    "# load the trained model\n",
    "trained_flair_classifier = TextClassifier.load(flair_model_path)\n",
    "\n",
    "# evaluate the model\n",
    "flair_results = evaluate_model(trained_flair_classifier, training_corpus.test, output_csv_path=\"../src/outputs/flair_model_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the best model\n",
    "roberta_model_path = \"../flair_models/roberta_model/best-model.pt\"\n",
    "\n",
    "# load the trained model\n",
    "trained_roberta_classifier = TextClassifier.load(roberta_model_path)\n",
    "\n",
    "# evaluate the model\n",
    "roberta_results = evaluate_model(trained_roberta_classifier, training_corpus.test, output_csv_path=\"../src/outputs/roberta_model_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with new test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DataProcessor(output_dir=flair_data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to ../data/flair_data/evaluation.txt\n"
     ]
    }
   ],
   "source": [
    "# path to the CSV file\n",
    "csv_path = \"../data/test_data_movie.csv\"\n",
    "\n",
    "# process CSV for evaluation\n",
    "evaluation_data = processor.read_csv(csv_path, label_column_index=1, text_column_index=0)\n",
    "\n",
    "# save evaluation dataset\n",
    "processor.save_data(evaluation_data, 'evaluation.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-11 22:55:48,800 Reading data from ../data/flair_data\n",
      "2024-12-11 22:55:48,801 Train: ../data/flair_data/train_small.txt\n",
      "2024-12-11 22:55:48,802 Dev: ../data/flair_data/dev_small.txt\n",
      "2024-12-11 22:55:48,803 Test: ../data/flair_data/evaluation.txt\n",
      "2024-12-11 22:55:49,161 Initialized corpus ../data/flair_data (label type name is 'sentiment')\n",
      "Number of evaluation sentences: 40000\n"
     ]
    }
   ],
   "source": [
    "# create evaluation corpus\n",
    "evaluation_corpus = ClassificationCorpus(\n",
    "    corpus_folder,\n",
    "    test_file='evaluation.txt',\n",
    "    label_type='sentiment'\n",
    ")\n",
    "\n",
    "# double check statistics\n",
    "print(f\"Number of evaluation sentences: {len(evaluation_corpus.test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the best model\n",
    "flair_distilbert_model_path = \"../flair_models/flair_distilbert_model/best-model.pt\"\n",
    "\n",
    "# load the trained model\n",
    "trained_flair_distilbert_classifier = TextClassifier.load(flair_distilbert_model_path)\n",
    "\n",
    "# evaluate the model\n",
    "flair_distilbert_evaluation_results = evaluate_model(trained_flair_distilbert_classifier, evaluation_corpus.test, output_csv_path=\"../src/outputs/flair_distilbert_model_evaluation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the best model\n",
    "flair_roberta_model_path = \"../flair_models/flair_roberta_model/best-model.pt\"\n",
    "\n",
    "# load the trained model\n",
    "trained_flair_roberta_classifier = TextClassifier.load(flair_roberta_model_path)\n",
    "\n",
    "# evaluate the model\n",
    "flair_roberta_evaluation_results = evaluate_model(trained_flair_roberta_classifier, evaluation_corpus.test, output_csv_path=\"../src/outputs/flair_roberta_model_evaluation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the best model\n",
    "flair_model_path = \"../flair_models/flair_model/best-model.pt\"\n",
    "\n",
    "# load the trained model\n",
    "trained_flair_classifier = TextClassifier.load(flair_model_path)\n",
    "\n",
    "# evaluate the model\n",
    "flair_evaluation_results = evaluate_model(trained_flair_classifier, evaluation_corpus.test, output_csv_path=\"../src/outputs/flair_model_evaluation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the best model\n",
    "roberta_model_path = \"../flair_models/roberta_model/best-model.pt\"\n",
    "\n",
    "# load the trained model\n",
    "trained_roberta_classifier = TextClassifier.load(roberta_model_path)\n",
    "\n",
    "# evaluate the model\n",
    "roberta_evaluation_results = evaluate_model(trained_roberta_classifier, evaluation_corpus.test, output_csv_path=\"../src/outputs/roberta_model_evaluation.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
