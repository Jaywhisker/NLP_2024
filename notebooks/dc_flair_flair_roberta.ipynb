{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from flair.data import Corpus, Sentence\n",
    "from flair.datasets import ClassificationCorpus\n",
    "from flair.embeddings import (\n",
    "    TransformerDocumentEmbeddings,\n",
    "    FlairEmbeddings, \n",
    "    DocumentRNNEmbeddings\n",
    ")\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.training_utils import EvaluationMetric\n",
    "from flair.visual.training_curves import Plotter\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from torch.optim import AdamW\n",
    "import logging\n",
    "\n",
    "import flair\n",
    "flair.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {flair.device}\")\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.2.1+cu121\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 11:42:53,689 Reading data from data\\flair_data\n",
      "2024-12-05 11:42:53,689 Train: data\\flair_data\\train.txt\n",
      "2024-12-05 11:42:53,689 Dev: data\\flair_data\\dev.txt\n",
      "2024-12-05 11:42:53,690 Test: data\\flair_data\\test.txt\n",
      "2024-12-05 11:42:54,355 Initialized corpus data\\flair_data (label type name is 'sentiment')\n",
      "Number of training sentences: 22500\n",
      "Number of validation sentences: 2500\n",
      "Number of test sentences: 25000\n",
      "2024-12-05 11:42:54,356 Reading data from data\\flair_data\n",
      "2024-12-05 11:42:54,356 Train: data\\flair_data\\train_small.txt\n",
      "2024-12-05 11:42:54,356 Dev: data\\flair_data\\dev_small.txt\n",
      "2024-12-05 11:42:54,357 Test: data\\flair_data\\test_small.txt\n",
      "2024-12-05 11:42:54,359 Initialized corpus data/flair_data (label type name is 'sentiment')\n",
      "Number of training sentences: 55\n",
      "Number of validation sentences: 25\n",
      "Number of test sentences: 48\n"
     ]
    }
   ],
   "source": [
    "# Ensure the output directory exists\n",
    "flair_data_folder = \"data/flair_data\"\n",
    "os.makedirs(flair_data_folder, exist_ok=True)\n",
    "\n",
    "# Define the folder where the data is located\n",
    "corpus_folder = Path(flair_data_folder)\n",
    "\n",
    "# Create the corpus\n",
    "corpus = ClassificationCorpus(\n",
    "    corpus_folder,\n",
    "    train_file='train.txt',\n",
    "    dev_file='dev.txt',\n",
    "    test_file='test.txt',\n",
    "    label_type='sentiment'\n",
    ")\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Number of training sentences: {len(corpus.train)}\")\n",
    "print(f\"Number of validation sentences: {len(corpus.dev)}\")\n",
    "print(f\"Number of test sentences: {len(corpus.test)}\")\n",
    "\n",
    "# Small dataset\n",
    "small_data_folder = \"data/flair_data\"\n",
    "small_corpus = ClassificationCorpus(small_data_folder, \n",
    "                                    train_file=\"train_small.txt\",  \n",
    "                                    dev_file='dev_small.txt', \n",
    "                                    test_file='test_small.txt',\n",
    "                                    label_type=\"sentiment\")\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Number of training sentences: {len(small_corpus.train)}\")\n",
    "print(f\"Number of validation sentences: {len(small_corpus.dev)}\")\n",
    "print(f\"Number of test sentences: {len(small_corpus.test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 11:42:54,515 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "22500it [00:55, 407.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 11:43:49,694 Dictionary created for label 'sentiment' with 2 values: POS (seen 11298 times), NEG (seen 11202 times)\n",
      "Dictionary with 2 tags: POS, NEG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "label_dict = corpus.make_label_dictionary(label_type='sentiment')\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef3aff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(classifier, test_dataset):\n",
    "    \"\"\"\n",
    "    Evaluate a Flair classifier on a given test dataset, with verification and debugging steps.\n",
    "\n",
    "    Args:\n",
    "        classifier (TextClassifier): The trained Flair classifier.\n",
    "        test_dataset (Dataset): The test dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    # Label mapping\n",
    "    label_mapping = {\"NEG\": 0, \"POS\": 1}\n",
    "\n",
    "    # Iterate over the test dataset with tqdm progress bar\n",
    "    for sentence in tqdm.tqdm(test_dataset, desc=\"Evaluating\", leave=True):\n",
    "        sentence.to(flair.device)\n",
    "\n",
    "        # Get true label\n",
    "        true_label = sentence.get_label(\"sentiment\").value\n",
    "        true_labels.append(true_label)\n",
    "\n",
    "        # Get predicted label\n",
    "        classifier.predict(sentence)\n",
    "        predicted_label = sentence.labels[0].value\n",
    "        predicted_labels.append(predicted_label)\n",
    "\n",
    "    # Verify label consistency\n",
    "    print(\"True Labels Sample:\", true_labels[:5])\n",
    "    print(\"Predicted Labels Sample:\", predicted_labels[:5])\n",
    "    print(\"True Label Distribution:\", Counter(true_labels))\n",
    "    print(\"Predicted Label Distribution:\", Counter(predicted_labels))\n",
    "\n",
    "    # Map labels to numeric values for sklearn\n",
    "    try:\n",
    "        true_labels_mapped = [label_mapping[label] for label in true_labels]\n",
    "        predicted_labels_mapped = [label_mapping[label] for label in predicted_labels]\n",
    "    except KeyError as e:\n",
    "        print(f\"Label mapping error: {e}. Ensure all labels are in {label_mapping}.\")\n",
    "        return {}\n",
    "\n",
    "    # Verify mapped labels\n",
    "    print(\"Mapped True Labels Sample:\", true_labels_mapped[:5])\n",
    "    print(\"Mapped Predicted Labels Sample:\", predicted_labels_mapped[:5])\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels_mapped, predicted_labels_mapped)\n",
    "    precision = precision_score(true_labels_mapped, predicted_labels_mapped, pos_label=1, zero_division=0)\n",
    "    recall = recall_score(true_labels_mapped, predicted_labels_mapped, pos_label=1, zero_division=0)\n",
    "    f1 = f1_score(true_labels_mapped, predicted_labels_mapped, pos_label=1, zero_division=0)\n",
    "\n",
    "    # Full classification report\n",
    "    classification_rep = classification_report(\n",
    "        true_labels_mapped,\n",
    "        predicted_labels_mapped,\n",
    "        target_names=[\"NEG\", \"POS\"]  # Match target names with label_mapping\n",
    "    )\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_rep)\n",
    "\n",
    "    # Return metrics as a dictionary\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"classification_report\": classification_rep,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flair embedding: to evaluate if code is working on small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Initialize Flair embeddings\n",
    "forward_embedding = FlairEmbeddings(\"news-forward\")\n",
    "backward_embedding = FlairEmbeddings(\"news-backward\")\n",
    "\n",
    "# Step 3: Create DocumentRNNEmbeddings using only Flair embeddings\n",
    "document_embeddings = DocumentRNNEmbeddings(\n",
    "    embeddings=[forward_embedding, backward_embedding],\n",
    "    hidden_size=256,  # Adjust based on your computational resources\n",
    "    reproject_words=True,  # Reproject word embeddings into a new space\n",
    "    reproject_words_dimension=256,  # Dimensionality of reprojection\n",
    ")\n",
    "\n",
    "flair_classifier = TextClassifier(\n",
    "    embeddings=document_embeddings,\n",
    "    label_dictionary=label_dict,\n",
    "    label_type=\"sentiment\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 18:53:56,071 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 18:53:56,071 Model: \"TextClassifier(\n",
      "  (embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4096, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 256, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=256, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (locked_dropout): LockedDropout(p=0.0)\n",
      "  (word_dropout): WordDropout(p=0.0)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2024-12-04 18:53:56,071 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 18:53:56,072 Corpus: 55 train + 25 dev + 48 test sentences\n",
      "2024-12-04 18:53:56,072 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 18:53:56,073 Train:  55 sentences\n",
      "2024-12-04 18:53:56,073         (train_with_dev=False, train_with_test=False)\n",
      "2024-12-04 18:53:56,073 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 18:53:56,073 Training Params:\n",
      "2024-12-04 18:53:56,073  - learning_rate: \"5e-05\" \n",
      "2024-12-04 18:53:56,074  - mini_batch_size: \"8\"\n",
      "2024-12-04 18:53:56,074  - max_epochs: \"3\"\n",
      "2024-12-04 18:53:56,074  - shuffle: \"True\"\n",
      "2024-12-04 18:53:56,074 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 18:53:56,075 Plugins:\n",
      "2024-12-04 18:53:56,075  - LinearScheduler | warmup_fraction: '0.1'\n",
      "2024-12-04 18:53:56,075 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 18:53:56,075 Final evaluation on model from best epoch (best-model.pt)\n",
      "2024-12-04 18:53:56,075  - metric: \"('micro avg', 'f1-score')\"\n",
      "2024-12-04 18:53:56,076 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 18:53:56,076 Computation:\n",
      "2024-12-04 18:53:56,076  - compute on device: cpu\n",
      "2024-12-04 18:53:56,076  - embedding storage: none\n",
      "2024-12-04 18:53:56,077 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 18:53:56,077 Model training base path: \"flair_flair_small_test_model\"\n",
      "2024-12-04 18:53:56,077 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 18:53:56,077 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiningonn/anaconda3/envs/nlp_project/lib/python3.8/site-packages/flair/trainers/trainer.py:499: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp and flair.device.type != \"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 18:54:17,774 epoch 1 - iter 1/7 - loss 0.69338685 - time (sec): 21.70 - samples/sec: 0.37 - lr: 0.000000 - momentum: 0.000000\n",
      "2024-12-04 18:54:34,932 epoch 1 - iter 2/7 - loss 0.71438470 - time (sec): 38.85 - samples/sec: 0.41 - lr: 0.000025 - momentum: 0.000000\n",
      "2024-12-04 18:54:50,813 epoch 1 - iter 3/7 - loss 0.70975542 - time (sec): 54.74 - samples/sec: 0.44 - lr: 0.000050 - momentum: 0.000000\n",
      "2024-12-04 18:55:03,183 epoch 1 - iter 4/7 - loss 0.69591980 - time (sec): 67.10 - samples/sec: 0.48 - lr: 0.000048 - momentum: 0.000000\n",
      "2024-12-04 18:55:08,366 epoch 1 - iter 5/7 - loss 0.69491612 - time (sec): 72.29 - samples/sec: 0.55 - lr: 0.000045 - momentum: 0.000000\n",
      "2024-12-04 18:55:16,929 epoch 1 - iter 6/7 - loss 0.69524315 - time (sec): 80.85 - samples/sec: 0.59 - lr: 0.000043 - momentum: 0.000000\n",
      "2024-12-04 18:55:37,075 epoch 1 - iter 7/7 - loss 0.68840644 - time (sec): 101.00 - samples/sec: 0.54 - lr: 0.000040 - momentum: 0.000000\n",
      "2024-12-04 18:55:37,076 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 18:55:37,077 EPOCH 1 done: loss 0.6884 - lr: 0.000040\n",
      "2024-12-04 18:55:37,077 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:28<00:00, 14.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 18:56:05,563 DEV : loss 0.6889662742614746 - f1-score (micro avg)  0.52\n",
      "2024-12-04 18:56:05,598 saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 18:56:05,783 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 18:56:26,566 epoch 2 - iter 1/7 - loss 0.66968471 - time (sec): 20.78 - samples/sec: 0.38 - lr: 0.000038 - momentum: 0.000000\n",
      "2024-12-04 18:56:36,561 epoch 2 - iter 2/7 - loss 0.67560032 - time (sec): 30.78 - samples/sec: 0.52 - lr: 0.000036 - momentum: 0.000000\n",
      "2024-12-04 18:56:49,392 epoch 2 - iter 3/7 - loss 0.68243100 - time (sec): 43.61 - samples/sec: 0.55 - lr: 0.000033 - momentum: 0.000000\n",
      "2024-12-04 18:56:58,120 epoch 2 - iter 4/7 - loss 0.68006256 - time (sec): 52.34 - samples/sec: 0.61 - lr: 0.000031 - momentum: 0.000000\n",
      "2024-12-04 18:57:05,295 epoch 2 - iter 5/7 - loss 0.67606875 - time (sec): 59.51 - samples/sec: 0.67 - lr: 0.000029 - momentum: 0.000000\n",
      "2024-12-04 18:57:22,388 epoch 2 - iter 6/7 - loss 0.67890920 - time (sec): 76.61 - samples/sec: 0.63 - lr: 0.000026 - momentum: 0.000000\n",
      "2024-12-04 18:57:44,074 epoch 2 - iter 7/7 - loss 0.68004244 - time (sec): 98.29 - samples/sec: 0.56 - lr: 0.000024 - momentum: 0.000000\n",
      "2024-12-04 18:57:44,075 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 18:57:44,076 EPOCH 2 done: loss 0.6800 - lr: 0.000024\n",
      "2024-12-04 18:57:44,076 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:28<00:00, 14.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 18:58:12,480 DEV : loss 0.6882741451263428 - f1-score (micro avg)  0.52\n",
      "2024-12-04 18:58:12,514 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 18:58:34,363 epoch 3 - iter 1/7 - loss 0.64284366 - time (sec): 21.85 - samples/sec: 0.37 - lr: 0.000021 - momentum: 0.000000\n",
      "2024-12-04 18:58:41,430 epoch 3 - iter 2/7 - loss 0.66006476 - time (sec): 28.92 - samples/sec: 0.55 - lr: 0.000019 - momentum: 0.000000\n",
      "2024-12-04 18:58:58,659 epoch 3 - iter 3/7 - loss 0.66148293 - time (sec): 46.14 - samples/sec: 0.52 - lr: 0.000017 - momentum: 0.000000\n",
      "2024-12-04 18:59:18,973 epoch 3 - iter 4/7 - loss 0.66381870 - time (sec): 66.46 - samples/sec: 0.48 - lr: 0.000014 - momentum: 0.000000\n",
      "2024-12-04 18:59:31,865 epoch 3 - iter 5/7 - loss 0.67462469 - time (sec): 79.35 - samples/sec: 0.50 - lr: 0.000012 - momentum: 0.000000\n",
      "2024-12-04 18:59:47,719 epoch 3 - iter 6/7 - loss 0.67824361 - time (sec): 95.20 - samples/sec: 0.50 - lr: 0.000010 - momentum: 0.000000\n",
      "2024-12-04 18:59:56,358 epoch 3 - iter 7/7 - loss 0.67673341 - time (sec): 103.84 - samples/sec: 0.53 - lr: 0.000007 - momentum: 0.000000\n",
      "2024-12-04 18:59:56,359 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 18:59:56,359 EPOCH 3 done: loss 0.6767 - lr: 0.000007\n",
      "2024-12-04 18:59:56,360 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:28<00:00, 14.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 19:00:24,754 DEV : loss 0.686703085899353 - f1-score (micro avg)  0.56\n",
      "2024-12-04 19:00:24,789 saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 19:00:25,076 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 19:00:25,077 Loading model from best epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:05<00:00, 21.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 19:01:30,524 \n",
      "Results:\n",
      "- F-score (micro) 0.5\n",
      "- F-score (macro) 0.395\n",
      "- Accuracy 0.5\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NEG     0.5238    0.8462    0.6471        26\n",
      "         POS     0.3333    0.0909    0.1429        22\n",
      "\n",
      "    accuracy                         0.5000        48\n",
      "   macro avg     0.4286    0.4685    0.3950        48\n",
      "weighted avg     0.4365    0.5000    0.4160        48\n",
      "\n",
      "2024-12-04 19:01:30,524 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.5}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the logging level to INFO\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = ModelTrainer(flair_classifier, small_corpus)\n",
    "\n",
    "#Fine-tune the model\n",
    "trainer.fine_tune(\n",
    "    base_path='flair_flair_small_test_model',             # Directory to save the model and logs\n",
    "    learning_rate=5e-5,                  # Learning rate for fine-tuning\n",
    "    mini_batch_size=8,                   # Smaller batch size for transformers\n",
    "    max_epochs=3,                        # Number of epochs\n",
    "    embeddings_storage_mode='none',     \n",
    "    optimizer=AdamW,                     # Optimizer suited for transformers\n",
    "    save_final_model=True,               # Save the final model\n",
    "    save_model_each_k_epochs=1,          # Save model checkpoint every epoch\n",
    "    create_file_logs=True,               # Save logs to a file\n",
    "    create_loss_file=True,               # Save loss values to a file\n",
    "    use_final_model_for_eval=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 48/48 [01:33<00:00,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Labels Sample: ['NEG', 'NEG', 'NEG', 'NEG', 'NEG']\n",
      "Predicted Labels Sample: ['NEG', 'NEG', 'NEG', 'NEG', 'NEG']\n",
      "True Label Distribution: Counter({'NEG': 26, 'POS': 22})\n",
      "Predicted Label Distribution: Counter({'NEG': 42, 'POS': 6})\n",
      "Mapped True Labels Sample: [0, 0, 0, 0, 0]\n",
      "Mapped Predicted Labels Sample: [0, 0, 0, 0, 0]\n",
      "\n",
      "Accuracy: 0.5000\n",
      "Precision: 0.3333\n",
      "Recall: 0.0909\n",
      "F1 Score: 0.1429\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NEG       0.52      0.85      0.65        26\n",
      "         POS       0.33      0.09      0.14        22\n",
      "\n",
      "    accuracy                           0.50        48\n",
      "   macro avg       0.43      0.47      0.39        48\n",
      "weighted avg       0.44      0.50      0.42        48\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to the saved model\n",
    "saved_model_path = \"flair_flair_small_test_model/best-model.pt\"  # Update this if the path or filename is different\n",
    "\n",
    "# Load the trained model\n",
    "flair_small_classifier = TextClassifier.load(saved_model_path).to(flair.device)\n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_model(flair_small_classifier, small_corpus.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flair embedding: actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 19:23:03,117 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 19:23:03,118 Model: \"TextClassifier(\n",
      "  (embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4096, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 256, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=256, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (locked_dropout): LockedDropout(p=0.0)\n",
      "  (word_dropout): WordDropout(p=0.0)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2024-12-04 19:23:03,120 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 19:23:03,120 Corpus: 22500 train + 2500 dev + 25000 test sentences\n",
      "2024-12-04 19:23:03,121 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 19:23:03,121 Train:  22500 sentences\n",
      "2024-12-04 19:23:03,121         (train_with_dev=False, train_with_test=False)\n",
      "2024-12-04 19:23:03,122 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 19:23:03,122 Training Params:\n",
      "2024-12-04 19:23:03,123  - learning_rate: \"5e-05\" \n",
      "2024-12-04 19:23:03,123  - mini_batch_size: \"8\"\n",
      "2024-12-04 19:23:03,123  - max_epochs: \"10\"\n",
      "2024-12-04 19:23:03,123  - shuffle: \"True\"\n",
      "2024-12-04 19:23:03,124 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 19:23:03,124 Plugins:\n",
      "2024-12-04 19:23:03,125  - LinearScheduler | warmup_fraction: '0.1'\n",
      "2024-12-04 19:23:03,125 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 19:23:03,125 Final evaluation on model from best epoch (best-model.pt)\n",
      "2024-12-04 19:23:03,126  - metric: \"('micro avg', 'f1-score')\"\n",
      "2024-12-04 19:23:03,126 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 19:23:03,127 Computation:\n",
      "2024-12-04 19:23:03,127  - compute on device: cuda\n",
      "2024-12-04 19:23:03,127  - embedding storage: gpu\n",
      "2024-12-04 19:23:03,128 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 19:23:03,128 Model training base path: \"flair_transformer_model\"\n",
      "2024-12-04 19:23:03,128 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 19:23:03,129 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 19:27:29,928 epoch 1 - iter 281/2813 - loss 0.69614082 - time (sec): 266.80 - samples/sec: 8.43 - lr: 0.000005 - momentum: 0.000000\n",
      "2024-12-04 19:32:11,706 epoch 1 - iter 562/2813 - loss 0.69414370 - time (sec): 548.58 - samples/sec: 8.20 - lr: 0.000010 - momentum: 0.000000\n",
      "2024-12-04 19:36:43,170 epoch 1 - iter 843/2813 - loss 0.69294460 - time (sec): 820.04 - samples/sec: 8.22 - lr: 0.000015 - momentum: 0.000000\n",
      "2024-12-04 19:41:05,104 epoch 1 - iter 1124/2813 - loss 0.69094028 - time (sec): 1081.98 - samples/sec: 8.31 - lr: 0.000020 - momentum: 0.000000\n",
      "2024-12-04 19:45:32,261 epoch 1 - iter 1405/2813 - loss 0.68832208 - time (sec): 1349.13 - samples/sec: 8.33 - lr: 0.000025 - momentum: 0.000000\n",
      "2024-12-04 19:50:09,156 epoch 1 - iter 1686/2813 - loss 0.68516759 - time (sec): 1626.03 - samples/sec: 8.30 - lr: 0.000030 - momentum: 0.000000\n",
      "2024-12-04 19:54:36,960 epoch 1 - iter 1967/2813 - loss 0.68084142 - time (sec): 1893.83 - samples/sec: 8.31 - lr: 0.000035 - momentum: 0.000000\n",
      "2024-12-04 19:59:06,988 epoch 1 - iter 2248/2813 - loss 0.67327996 - time (sec): 2163.86 - samples/sec: 8.31 - lr: 0.000040 - momentum: 0.000000\n",
      "2024-12-04 20:03:50,579 epoch 1 - iter 2529/2813 - loss 0.66613335 - time (sec): 2447.45 - samples/sec: 8.27 - lr: 0.000045 - momentum: 0.000000\n",
      "2024-12-04 20:08:32,557 epoch 1 - iter 2810/2813 - loss 0.66381250 - time (sec): 2729.43 - samples/sec: 8.24 - lr: 0.000050 - momentum: 0.000000\n",
      "2024-12-04 20:08:34,909 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 20:08:34,910 EPOCH 1 done: loss 0.6637 - lr: 0.000050\n",
      "2024-12-04 20:08:34,910 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [24:14<00:00,  9.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 20:32:49,182 DEV : loss 0.5398534536361694 - f1-score (micro avg)  0.7296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 20:32:55,899 saving best model\n",
      "2024-12-04 20:32:56,044 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 20:43:00,450 epoch 2 - iter 281/2813 - loss 0.56741641 - time (sec): 604.41 - samples/sec: 3.72 - lr: 0.000049 - momentum: 0.000000\n",
      "2024-12-04 20:50:01,202 epoch 2 - iter 562/2813 - loss 0.56086464 - time (sec): 1025.16 - samples/sec: 4.39 - lr: 0.000049 - momentum: 0.000000\n",
      "2024-12-04 20:56:53,066 epoch 2 - iter 843/2813 - loss 0.54737712 - time (sec): 1437.02 - samples/sec: 4.69 - lr: 0.000048 - momentum: 0.000000\n",
      "2024-12-04 21:03:19,606 epoch 2 - iter 1124/2813 - loss 0.54221408 - time (sec): 1823.56 - samples/sec: 4.93 - lr: 0.000048 - momentum: 0.000000\n",
      "2024-12-04 21:08:58,776 epoch 2 - iter 1405/2813 - loss 0.54341581 - time (sec): 2162.73 - samples/sec: 5.20 - lr: 0.000047 - momentum: 0.000000\n",
      "2024-12-04 21:14:56,166 epoch 2 - iter 1686/2813 - loss 0.53647269 - time (sec): 2520.12 - samples/sec: 5.35 - lr: 0.000047 - momentum: 0.000000\n",
      "2024-12-04 21:20:53,130 epoch 2 - iter 1967/2813 - loss 0.53634152 - time (sec): 2877.09 - samples/sec: 5.47 - lr: 0.000046 - momentum: 0.000000\n",
      "2024-12-04 21:26:49,941 epoch 2 - iter 2248/2813 - loss 0.52946361 - time (sec): 3233.90 - samples/sec: 5.56 - lr: 0.000046 - momentum: 0.000000\n",
      "2024-12-04 21:32:57,607 epoch 2 - iter 2529/2813 - loss 0.52778789 - time (sec): 3601.56 - samples/sec: 5.62 - lr: 0.000045 - momentum: 0.000000\n",
      "2024-12-04 21:39:15,619 epoch 2 - iter 2810/2813 - loss 0.52860298 - time (sec): 3979.57 - samples/sec: 5.65 - lr: 0.000044 - momentum: 0.000000\n",
      "2024-12-04 21:39:19,418 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 21:39:19,419 EPOCH 2 done: loss 0.5284 - lr: 0.000044\n",
      "2024-12-04 21:39:19,419 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [30:57<00:00, 11.83s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 22:10:17,480 DEV : loss 0.48584309220314026 - f1-score (micro avg)  0.7808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 22:10:23,991 saving best model\n",
      "2024-12-04 22:10:24,219 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 22:15:41,810 epoch 3 - iter 281/2813 - loss 0.50164425 - time (sec): 317.59 - samples/sec: 7.08 - lr: 0.000044 - momentum: 0.000000\n",
      "2024-12-04 22:20:43,911 epoch 3 - iter 562/2813 - loss 0.49488215 - time (sec): 619.69 - samples/sec: 7.26 - lr: 0.000043 - momentum: 0.000000\n",
      "2024-12-04 22:25:53,694 epoch 3 - iter 843/2813 - loss 0.48452236 - time (sec): 929.47 - samples/sec: 7.26 - lr: 0.000043 - momentum: 0.000000\n",
      "2024-12-04 22:31:21,616 epoch 3 - iter 1124/2813 - loss 0.48220646 - time (sec): 1257.40 - samples/sec: 7.15 - lr: 0.000042 - momentum: 0.000000\n",
      "2024-12-04 22:36:44,266 epoch 3 - iter 1405/2813 - loss 0.48683788 - time (sec): 1580.05 - samples/sec: 7.11 - lr: 0.000042 - momentum: 0.000000\n",
      "2024-12-04 22:42:36,763 epoch 3 - iter 1686/2813 - loss 0.48298356 - time (sec): 1932.54 - samples/sec: 6.98 - lr: 0.000041 - momentum: 0.000000\n",
      "2024-12-04 22:47:12,706 epoch 3 - iter 1967/2813 - loss 0.47962306 - time (sec): 2208.49 - samples/sec: 7.13 - lr: 0.000041 - momentum: 0.000000\n",
      "2024-12-04 22:52:48,441 epoch 3 - iter 2248/2813 - loss 0.47600314 - time (sec): 2544.22 - samples/sec: 7.07 - lr: 0.000040 - momentum: 0.000000\n",
      "2024-12-04 23:01:13,346 epoch 3 - iter 2529/2813 - loss 0.47599779 - time (sec): 3049.13 - samples/sec: 6.64 - lr: 0.000039 - momentum: 0.000000\n",
      "2024-12-04 23:06:25,550 epoch 3 - iter 2810/2813 - loss 0.47502581 - time (sec): 3361.33 - samples/sec: 6.69 - lr: 0.000039 - momentum: 0.000000\n",
      "2024-12-04 23:06:27,965 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 23:06:27,965 EPOCH 3 done: loss 0.4751 - lr: 0.000039\n",
      "2024-12-04 23:06:27,966 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [31:20<00:00, 11.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:37:48,166 DEV : loss 0.44944584369659424 - f1-score (micro avg)  0.8032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:37:55,052 saving best model\n",
      "2024-12-04 23:37:55,272 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 23:42:54,870 epoch 4 - iter 281/2813 - loss 0.45180363 - time (sec): 299.60 - samples/sec: 7.50 - lr: 0.000038 - momentum: 0.000000\n",
      "2024-12-04 23:48:15,175 epoch 4 - iter 562/2813 - loss 0.45040981 - time (sec): 619.90 - samples/sec: 7.25 - lr: 0.000038 - momentum: 0.000000\n",
      "2024-12-04 23:53:23,181 epoch 4 - iter 843/2813 - loss 0.45444247 - time (sec): 927.91 - samples/sec: 7.27 - lr: 0.000037 - momentum: 0.000000\n",
      "2024-12-04 23:58:33,535 epoch 4 - iter 1124/2813 - loss 0.45829335 - time (sec): 1238.26 - samples/sec: 7.26 - lr: 0.000037 - momentum: 0.000000\n",
      "2024-12-05 00:04:10,761 epoch 4 - iter 1405/2813 - loss 0.45855153 - time (sec): 1575.49 - samples/sec: 7.13 - lr: 0.000036 - momentum: 0.000000\n",
      "2024-12-05 00:09:40,754 epoch 4 - iter 1686/2813 - loss 0.45490447 - time (sec): 1905.48 - samples/sec: 7.08 - lr: 0.000036 - momentum: 0.000000\n",
      "2024-12-05 00:14:45,997 epoch 4 - iter 1967/2813 - loss 0.45344166 - time (sec): 2210.72 - samples/sec: 7.12 - lr: 0.000035 - momentum: 0.000000\n",
      "2024-12-05 00:20:17,207 epoch 4 - iter 2248/2813 - loss 0.45283457 - time (sec): 2541.93 - samples/sec: 7.07 - lr: 0.000034 - momentum: 0.000000\n",
      "2024-12-05 00:25:27,253 epoch 4 - iter 2529/2813 - loss 0.45259542 - time (sec): 2851.98 - samples/sec: 7.09 - lr: 0.000034 - momentum: 0.000000\n",
      "2024-12-05 00:30:31,644 epoch 4 - iter 2810/2813 - loss 0.45031311 - time (sec): 3156.37 - samples/sec: 7.12 - lr: 0.000033 - momentum: 0.000000\n",
      "2024-12-05 00:30:33,673 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 00:30:33,673 EPOCH 4 done: loss 0.4502 - lr: 0.000033\n",
      "2024-12-05 00:30:33,673 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [18:16<00:00,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 00:48:50,565 DEV : loss 0.46021169424057007 - f1-score (micro avg)  0.8092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 00:48:57,058 saving best model\n",
      "2024-12-05 00:48:57,225 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 00:53:21,823 epoch 5 - iter 281/2813 - loss 0.43247064 - time (sec): 264.60 - samples/sec: 8.50 - lr: 0.000033 - momentum: 0.000000\n",
      "2024-12-05 00:57:42,689 epoch 5 - iter 562/2813 - loss 0.42157177 - time (sec): 525.46 - samples/sec: 8.56 - lr: 0.000032 - momentum: 0.000000\n",
      "2024-12-05 01:02:09,619 epoch 5 - iter 843/2813 - loss 0.42211544 - time (sec): 792.39 - samples/sec: 8.51 - lr: 0.000032 - momentum: 0.000000\n",
      "2024-12-05 01:06:28,720 epoch 5 - iter 1124/2813 - loss 0.42720328 - time (sec): 1051.49 - samples/sec: 8.55 - lr: 0.000031 - momentum: 0.000000\n",
      "2024-12-05 01:10:51,207 epoch 5 - iter 1405/2813 - loss 0.42549192 - time (sec): 1313.98 - samples/sec: 8.55 - lr: 0.000031 - momentum: 0.000000\n",
      "2024-12-05 01:14:54,121 epoch 5 - iter 1686/2813 - loss 0.43022526 - time (sec): 1556.90 - samples/sec: 8.66 - lr: 0.000030 - momentum: 0.000000\n",
      "2024-12-05 01:19:22,396 epoch 5 - iter 1967/2813 - loss 0.42915069 - time (sec): 1825.17 - samples/sec: 8.62 - lr: 0.000029 - momentum: 0.000000\n",
      "2024-12-05 01:23:51,599 epoch 5 - iter 2248/2813 - loss 0.42815842 - time (sec): 2094.37 - samples/sec: 8.59 - lr: 0.000029 - momentum: 0.000000\n",
      "2024-12-05 01:28:19,899 epoch 5 - iter 2529/2813 - loss 0.42447582 - time (sec): 2362.67 - samples/sec: 8.56 - lr: 0.000028 - momentum: 0.000000\n",
      "2024-12-05 01:32:55,092 epoch 5 - iter 2810/2813 - loss 0.42309438 - time (sec): 2637.87 - samples/sec: 8.52 - lr: 0.000028 - momentum: 0.000000\n",
      "2024-12-05 01:32:57,850 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 01:32:57,850 EPOCH 5 done: loss 0.4229 - lr: 0.000028\n",
      "2024-12-05 01:32:57,851 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [06:55<00:00,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 01:39:53,353 DEV : loss 0.4273413419723511 - f1-score (micro avg)  0.7988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 01:39:59,715 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 01:40:03,464 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 01:40:03,464 Exiting from training early.\n",
      "2024-12-05 01:40:03,465 Saving model ...\n",
      "2024-12-05 01:40:03,678 Done.\n",
      "2024-12-05 01:40:03,679 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 01:40:03,679 Loading model from best epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1563 [00:15<36:29,  1.41s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ModelTrainer(flair_classifier, corpus)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#Fine-tune the model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfine_tune\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflair_transformer_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# Directory to save the model and logs\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# Learning rate for fine-tuning\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmini_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;66;43;03m# Smaller batch size for transformers\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                        \u001b[49m\u001b[38;5;66;43;03m# Number of epochs\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings_storage_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAdamW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m# Optimizer suited for transformers\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_final_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Save the final model\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_model_each_k_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# Save model checkpoint every epoch\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_file_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Save logs to a file\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_loss_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Save loss values to a file\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_final_model_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\flair\\trainers\\trainer.py:253\u001b[0m, in \u001b[0;36mModelTrainer.fine_tune\u001b[1;34m(self, base_path, warmup_fraction, learning_rate, decoder_learning_rate, mini_batch_size, eval_batch_size, mini_batch_chunk_size, max_epochs, optimizer, train_with_dev, train_with_test, reduce_transformer_vocab, main_evaluation_metric, monitor_test, monitor_train_sample, use_final_model_for_eval, gold_label_dictionary_for_eval, exclude_labels, sampler, shuffle, shuffle_first_epoch, embeddings_storage_mode, epoch, save_final_model, save_optimizer_state, save_model_each_k_epochs, create_file_logs, create_loss_file, write_weights, use_amp, plugins, attach_default_scheduler, **kwargs)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attach_default_scheduler:\n\u001b[0;32m    251\u001b[0m     plugins\u001b[38;5;241m.\u001b[39mappend(LinearSchedulerPlugin(warmup_fraction\u001b[38;5;241m=\u001b[39mwarmup_fraction))\n\u001b[1;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_custom(\n\u001b[0;32m    254\u001b[0m     base_path\u001b[38;5;241m=\u001b[39mbase_path,\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;66;03m# training parameters\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[0;32m    257\u001b[0m     decoder_learning_rate\u001b[38;5;241m=\u001b[39mdecoder_learning_rate,\n\u001b[0;32m    258\u001b[0m     mini_batch_size\u001b[38;5;241m=\u001b[39mmini_batch_size,\n\u001b[0;32m    259\u001b[0m     eval_batch_size\u001b[38;5;241m=\u001b[39meval_batch_size,\n\u001b[0;32m    260\u001b[0m     mini_batch_chunk_size\u001b[38;5;241m=\u001b[39mmini_batch_chunk_size,\n\u001b[0;32m    261\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mmax_epochs,\n\u001b[0;32m    262\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m    263\u001b[0m     train_with_dev\u001b[38;5;241m=\u001b[39mtrain_with_dev,\n\u001b[0;32m    264\u001b[0m     train_with_test\u001b[38;5;241m=\u001b[39mtrain_with_test,\n\u001b[0;32m    265\u001b[0m     reduce_transformer_vocab\u001b[38;5;241m=\u001b[39mreduce_transformer_vocab,\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;66;03m# evaluation and monitoring\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     main_evaluation_metric\u001b[38;5;241m=\u001b[39mmain_evaluation_metric,\n\u001b[0;32m    268\u001b[0m     monitor_test\u001b[38;5;241m=\u001b[39mmonitor_test,\n\u001b[0;32m    269\u001b[0m     monitor_train_sample\u001b[38;5;241m=\u001b[39mmonitor_train_sample,\n\u001b[0;32m    270\u001b[0m     use_final_model_for_eval\u001b[38;5;241m=\u001b[39muse_final_model_for_eval,\n\u001b[0;32m    271\u001b[0m     gold_label_dictionary_for_eval\u001b[38;5;241m=\u001b[39mgold_label_dictionary_for_eval,\n\u001b[0;32m    272\u001b[0m     exclude_labels\u001b[38;5;241m=\u001b[39mexclude_labels,\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;66;03m# sampling and shuffling\u001b[39;00m\n\u001b[0;32m    274\u001b[0m     sampler\u001b[38;5;241m=\u001b[39msampler,\n\u001b[0;32m    275\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[0;32m    276\u001b[0m     shuffle_first_epoch\u001b[38;5;241m=\u001b[39mshuffle_first_epoch,\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;66;03m# evaluation and monitoring\u001b[39;00m\n\u001b[0;32m    278\u001b[0m     embeddings_storage_mode\u001b[38;5;241m=\u001b[39membeddings_storage_mode,\n\u001b[0;32m    279\u001b[0m     epoch\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;66;03m# when and what to save\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     save_final_model\u001b[38;5;241m=\u001b[39msave_final_model,\n\u001b[0;32m    282\u001b[0m     save_optimizer_state\u001b[38;5;241m=\u001b[39msave_optimizer_state,\n\u001b[0;32m    283\u001b[0m     save_model_each_k_epochs\u001b[38;5;241m=\u001b[39msave_model_each_k_epochs,\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;66;03m# logging parameters\u001b[39;00m\n\u001b[0;32m    285\u001b[0m     create_file_logs\u001b[38;5;241m=\u001b[39mcreate_file_logs,\n\u001b[0;32m    286\u001b[0m     create_loss_file\u001b[38;5;241m=\u001b[39mcreate_loss_file,\n\u001b[0;32m    287\u001b[0m     write_weights\u001b[38;5;241m=\u001b[39mwrite_weights,\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;66;03m# amp\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     use_amp\u001b[38;5;241m=\u001b[39muse_amp,\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;66;03m# plugins\u001b[39;00m\n\u001b[0;32m    291\u001b[0m     plugins\u001b[38;5;241m=\u001b[39mplugins,\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    293\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\flair\\trainers\\trainer.py:785\u001b[0m, in \u001b[0;36mModelTrainer.train_custom\u001b[1;34m(self, base_path, learning_rate, decoder_learning_rate, mini_batch_size, eval_batch_size, mini_batch_chunk_size, max_epochs, optimizer, train_with_dev, train_with_test, max_grad_norm, reduce_transformer_vocab, main_evaluation_metric, monitor_test, monitor_train_sample, use_final_model_for_eval, gold_label_dictionary_for_eval, exclude_labels, sampler, shuffle, shuffle_first_epoch, embeddings_storage_mode, epoch, save_final_model, save_optimizer_state, save_model_each_k_epochs, create_file_logs, create_loss_file, write_weights, use_amp, plugins, **kwargs)\u001b[0m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    783\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting using last state of model ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 785\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgold_label_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmini_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest.tsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_storage_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmain_evaluation_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_evaluation_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgold_label_dictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgold_label_dictionary_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    797\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(test_results\u001b[38;5;241m.\u001b[39mdetailed_results)\n\u001b[0;32m    798\u001b[0m log_line(log)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\flair\\nn\\model.py:297\u001b[0m, in \u001b[0;36mClassifier.evaluate\u001b[1;34m(self, data_points, gold_label_type, out_path, embedding_storage_mode, mini_batch_size, main_evaluation_metric, exclude_labels, gold_label_dictionary, return_loss, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m     datapoint\u001b[38;5;241m.\u001b[39mremove_labels(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;66;03m# predict for batch\u001b[39;00m\n\u001b[1;32m--> 297\u001b[0m loss_and_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_storage_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_storage_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmini_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmini_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredicted\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_loss:\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loss_and_count, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\flair\\nn\\model.py:857\u001b[0m, in \u001b[0;36mDefaultClassifier.predict\u001b[1;34m(self, sentences, mini_batch_size, return_probabilities_for_all_classes, verbose, label_name, return_loss, embedding_storage_mode)\u001b[0m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;66;03m# pass data points through network and decode\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m data_point_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_data_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    858\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(data_point_tensor)\n\u001b[0;32m    859\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mask_scores(scores, data_points)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\flair\\nn\\model.py:726\u001b[0m, in \u001b[0;36mDefaultClassifier._encode_data_points\u001b[1;34m(self, sentences, data_points)\u001b[0m\n\u001b[0;32m    723\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_data_points\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentences: List[DT], data_points: List[DT2]):\n\u001b[0;32m    724\u001b[0m     \u001b[38;5;66;03m# embed sentences\u001b[39;00m\n\u001b[0;32m    725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_embed_sentence:\n\u001b[1;32m--> 726\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;66;03m# get a tensor of data points\u001b[39;00m\n\u001b[0;32m    729\u001b[0m     data_point_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_embedding_for_data_point(data_point) \u001b[38;5;28;01mfor\u001b[39;00m data_point \u001b[38;5;129;01min\u001b[39;00m data_points])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\flair\\embeddings\\base.py:50\u001b[0m, in \u001b[0;36mEmbeddings.embed\u001b[1;34m(self, data_points)\u001b[0m\n\u001b[0;32m     47\u001b[0m     data_points \u001b[38;5;241m=\u001b[39m [data_points]\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_everything_embedded(data_points):\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_embeddings_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_points\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\flair\\embeddings\\document.py:333\u001b[0m, in \u001b[0;36mDocumentRNNEmbeddings._add_embeddings_internal\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m# embed words in the sentence\u001b[39;00m\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m lengths: List[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(sentence\u001b[38;5;241m.\u001b[39mtokens) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences]\n\u001b[0;32m    336\u001b[0m longest_token_sequence_in_batch: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(lengths)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\flair\\embeddings\\token.py:97\u001b[0m, in \u001b[0;36mStackedEmbeddings.embed\u001b[1;34m(self, sentences, static_embeddings)\u001b[0m\n\u001b[0;32m     94\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [sentences]\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings:\n\u001b[1;32m---> 97\u001b[0m     \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\flair\\embeddings\\base.py:50\u001b[0m, in \u001b[0;36mEmbeddings.embed\u001b[1;34m(self, data_points)\u001b[0m\n\u001b[0;32m     47\u001b[0m     data_points \u001b[38;5;241m=\u001b[39m [data_points]\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_everything_embedded(data_points):\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_embeddings_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_points\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\flair\\embeddings\\token.py:815\u001b[0m, in \u001b[0;36mFlairEmbeddings._add_embeddings_internal\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m    812\u001b[0m end_marker \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;66;03m# get hidden states from language model\u001b[39;00m\n\u001b[1;32m--> 815\u001b[0m all_hidden_states_in_lm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_representation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_marker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_marker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchars_per_chunk\u001b[49m\n\u001b[0;32m    817\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfine_tune:\n\u001b[0;32m    820\u001b[0m     all_hidden_states_in_lm \u001b[38;5;241m=\u001b[39m all_hidden_states_in_lm\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\flair\\models\\language_model.py:160\u001b[0m, in \u001b[0;36mLanguageModel.get_representation\u001b[1;34m(self, strings, start_marker, end_marker, chars_per_chunk)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[0;32m    159\u001b[0m     batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 160\u001b[0m     rnn_output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m     output_parts\u001b[38;5;241m.\u001b[39mappend(rnn_output)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# concatenate all chunks to make final output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\flair\\models\\language_model.py:88\u001b[0m, in \u001b[0;36mLanguageModel.forward\u001b[1;34m(self, input, hidden, ordered_sequence_lengths, decode)\u001b[0m\n\u001b[0;32m     86\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m (h,)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\rnn.py:878\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    875\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 878\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    882\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set the logging level to INFO\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = ModelTrainer(flair_classifier, corpus)\n",
    "\n",
    "#Fine-tune the model\n",
    "trainer.fine_tune(\n",
    "    base_path='flair_transformer_model',             # Directory to save the model and logs\n",
    "    learning_rate=5e-5,                  # Learning rate for fine-tuning\n",
    "    mini_batch_size=8,                   # Smaller batch size for transformers\n",
    "    max_epochs=5,                        # Number of epochs\n",
    "    embeddings_storage_mode='gpu',     \n",
    "    optimizer=AdamW,                     # Optimizer suited for transformers\n",
    "    save_final_model=True,               # Save the final model\n",
    "    save_model_each_k_epochs=1,          # Save model checkpoint every epoch\n",
    "    create_file_logs=True,               # Save logs to a file\n",
    "    create_loss_file=True,               # Save loss values to a file\n",
    "    use_final_model_for_eval=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the saved model\n",
    "saved_model_path = \"flair_transformer_model/best-model.pt\"  # Update this if the path or filename is different\n",
    "\n",
    "# Load the trained model\n",
    "flair_classifier = TextClassifier.load(saved_model_path).to(flair.device)\n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_model(flair_classifier, corpus.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transformer classifier: to evaluate if code is working on small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize transformer embeddings\n",
    "transformer_embedding = TransformerDocumentEmbeddings(\n",
    "    model=\"roberta-base\",  # Transformer model of choice\n",
    "    fine_tune=True,        # Fine-tune the transformer model\n",
    "    layers=\"-1\",           # Use the last layer for representation\n",
    ")\n",
    "\n",
    "# Update classifier with transformer embedding\n",
    "transformer_classifier = TextClassifier(embeddings=transformer_embedding, label_dictionary=label_dict, label_type=\"sentiment\").to(flair.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b50412d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 19:10:38,394 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 19:10:38,395 Model: \"TextClassifier(\n",
      "  (embeddings): TransformerDocumentEmbeddings(\n",
      "    (model): RobertaModel(\n",
      "      (embeddings): RobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(50266, 768)\n",
      "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-11): 12 x RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): RobertaPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (locked_dropout): LockedDropout(p=0.0)\n",
      "  (word_dropout): WordDropout(p=0.0)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2024-12-04 19:10:38,395 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 19:10:38,395 Corpus: 55 train + 25 dev + 48 test sentences\n",
      "2024-12-04 19:10:38,396 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 19:10:38,396 Train:  55 sentences\n",
      "2024-12-04 19:10:38,397         (train_with_dev=False, train_with_test=False)\n",
      "2024-12-04 19:10:38,397 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 19:10:38,397 Training Params:\n",
      "2024-12-04 19:10:38,398  - learning_rate: \"5e-05\" \n",
      "2024-12-04 19:10:38,398  - mini_batch_size: \"8\"\n",
      "2024-12-04 19:10:38,398  - max_epochs: \"2\"\n",
      "2024-12-04 19:10:38,399  - shuffle: \"True\"\n",
      "2024-12-04 19:10:38,399 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 19:10:38,400 Plugins:\n",
      "2024-12-04 19:10:38,401  - LinearScheduler | warmup_fraction: '0.1'\n",
      "2024-12-04 19:10:38,401 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 19:10:38,402 Final evaluation on model from best epoch (best-model.pt)\n",
      "2024-12-04 19:10:38,402  - metric: \"('micro avg', 'f1-score')\"\n",
      "2024-12-04 19:10:38,402 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 19:10:38,403 Computation:\n",
      "2024-12-04 19:10:38,403  - compute on device: cpu\n",
      "2024-12-04 19:10:38,404  - embedding storage: gpu\n",
      "2024-12-04 19:10:38,405 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 19:10:38,405 Model training base path: \"flair_transformer_small_test_model\"\n",
      "2024-12-04 19:10:38,406 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 19:10:38,406 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiningonn/anaconda3/envs/nlp_project/lib/python3.8/site-packages/flair/trainers/trainer.py:499: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp and flair.device.type != \"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 19:10:44,409 epoch 1 - iter 1/7 - loss 1.04363453 - time (sec): 6.00 - samples/sec: 1.33 - lr: 0.000000 - momentum: 0.000000\n",
      "2024-12-04 19:10:47,945 epoch 1 - iter 2/7 - loss 0.90796769 - time (sec): 9.54 - samples/sec: 1.68 - lr: 0.000050 - momentum: 0.000000\n",
      "2024-12-04 19:10:53,608 epoch 1 - iter 3/7 - loss 0.79996492 - time (sec): 15.20 - samples/sec: 1.58 - lr: 0.000046 - momentum: 0.000000\n",
      "2024-12-04 19:10:57,539 epoch 1 - iter 4/7 - loss 0.77897805 - time (sec): 19.13 - samples/sec: 1.67 - lr: 0.000043 - momentum: 0.000000\n",
      "2024-12-04 19:11:03,182 epoch 1 - iter 5/7 - loss 0.77820672 - time (sec): 24.77 - samples/sec: 1.61 - lr: 0.000039 - momentum: 0.000000\n",
      "2024-12-04 19:11:08,492 epoch 1 - iter 6/7 - loss 0.79851161 - time (sec): 30.09 - samples/sec: 1.60 - lr: 0.000036 - momentum: 0.000000\n",
      "2024-12-04 19:11:12,155 epoch 1 - iter 7/7 - loss 0.79783030 - time (sec): 33.75 - samples/sec: 1.63 - lr: 0.000032 - momentum: 0.000000\n",
      "2024-12-04 19:11:12,156 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 19:11:12,157 EPOCH 1 done: loss 0.7978 - lr: 0.000032\n",
      "2024-12-04 19:11:12,157 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 19:11:16,578 DEV : loss 0.6676883697509766 - f1-score (micro avg)  0.64\n",
      "2024-12-04 19:11:16,612 saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 19:11:17,265 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 19:11:21,959 epoch 2 - iter 1/7 - loss 0.53675771 - time (sec): 4.69 - samples/sec: 1.70 - lr: 0.000029 - momentum: 0.000000\n",
      "2024-12-04 19:11:27,532 epoch 2 - iter 2/7 - loss 0.59745705 - time (sec): 10.26 - samples/sec: 1.56 - lr: 0.000025 - momentum: 0.000000\n",
      "2024-12-04 19:11:32,873 epoch 2 - iter 3/7 - loss 0.61329925 - time (sec): 15.61 - samples/sec: 1.54 - lr: 0.000021 - momentum: 0.000000\n",
      "2024-12-04 19:11:38,124 epoch 2 - iter 4/7 - loss 0.65516320 - time (sec): 20.86 - samples/sec: 1.53 - lr: 0.000018 - momentum: 0.000000\n",
      "2024-12-04 19:11:43,435 epoch 2 - iter 5/7 - loss 0.66054441 - time (sec): 26.17 - samples/sec: 1.53 - lr: 0.000014 - momentum: 0.000000\n",
      "2024-12-04 19:11:49,424 epoch 2 - iter 6/7 - loss 0.65232339 - time (sec): 32.16 - samples/sec: 1.49 - lr: 0.000011 - momentum: 0.000000\n",
      "2024-12-04 19:11:52,102 epoch 2 - iter 7/7 - loss 0.66314994 - time (sec): 34.84 - samples/sec: 1.58 - lr: 0.000007 - momentum: 0.000000\n",
      "2024-12-04 19:11:52,102 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 19:11:52,103 EPOCH 2 done: loss 0.6631 - lr: 0.000007\n",
      "2024-12-04 19:11:52,103 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 19:11:56,426 DEV : loss 0.6994249224662781 - f1-score (micro avg)  0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 19:11:57,119 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-04 19:11:57,120 Loading model from best epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:06<00:00,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 19:12:06,212 \n",
      "Results:\n",
      "- F-score (micro) 0.5625\n",
      "- F-score (macro) 0.3996\n",
      "- Accuracy 0.5625\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NEG     0.5532    1.0000    0.7123        26\n",
      "         POS     1.0000    0.0455    0.0870        22\n",
      "\n",
      "    accuracy                         0.5625        48\n",
      "   macro avg     0.7766    0.5227    0.3996        48\n",
      "weighted avg     0.7580    0.5625    0.4257        48\n",
      "\n",
      "2024-12-04 19:12:06,212 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.5625}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the logging level to INFO\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = ModelTrainer(transformer_classifier, small_corpus)\n",
    "\n",
    "#Fine-tune the model\n",
    "trainer.fine_tune(\n",
    "    base_path='flair_transformer_small_test_model',             # Directory to save the model and logs\n",
    "    learning_rate=5e-5,                  # Learning rate for fine-tuning\n",
    "    mini_batch_size=8,                   # Smaller batch size for transformers\n",
    "    max_epochs=2,                        # Number of epochs\n",
    "    embeddings_storage_mode='gpu',     \n",
    "    optimizer=AdamW,                     # Optimizer suited for transformers\n",
    "    save_final_model=True,               # Save the final model\n",
    "    save_model_each_k_epochs=1,          # Save model checkpoint every epoch\n",
    "    create_file_logs=True,               # Save logs to a file\n",
    "    create_loss_file=True,               # Save loss values to a file\n",
    "    use_final_model_for_eval=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 48/48 [00:03<00:00, 13.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Labels Sample: ['NEG', 'NEG', 'NEG', 'NEG', 'NEG']\n",
      "Predicted Labels Sample: ['NEG', 'NEG', 'NEG', 'NEG', 'NEG']\n",
      "True Label Distribution: Counter({'NEG': 26, 'POS': 22})\n",
      "Predicted Label Distribution: Counter({'NEG': 47, 'POS': 1})\n",
      "Mapped True Labels Sample: [0, 0, 0, 0, 0]\n",
      "Mapped Predicted Labels Sample: [0, 0, 0, 0, 0]\n",
      "\n",
      "Accuracy: 0.5625\n",
      "Precision: 1.0000\n",
      "Recall: 0.0455\n",
      "F1 Score: 0.0870\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NEG       0.55      1.00      0.71        26\n",
      "         POS       1.00      0.05      0.09        22\n",
      "\n",
      "    accuracy                           0.56        48\n",
      "   macro avg       0.78      0.52      0.40        48\n",
      "weighted avg       0.76      0.56      0.43        48\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to the saved model\n",
    "saved_model_path = \"flair_transformer_small_test_model/best-model.pt\"  # Update this if the path or filename is different\n",
    "\n",
    "# Load the trained model\n",
    "transformer_small_classifier = TextClassifier.load(saved_model_path).to(flair.device)\n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_model(transformer_small_classifier, small_corpus.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 10:54:03,697 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 10:54:03,698 Model: \"TextClassifier(\n",
      "  (embeddings): TransformerDocumentEmbeddings(\n",
      "    (model): RobertaModel(\n",
      "      (embeddings): RobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(50266, 768, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-11): 12 x RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSdpaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): RobertaPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (locked_dropout): LockedDropout(p=0.0)\n",
      "  (word_dropout): WordDropout(p=0.0)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2024-12-05 10:54:03,699 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 10:54:03,699 Corpus: 22500 train + 2500 dev + 25000 test sentences\n",
      "2024-12-05 10:54:03,700 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 10:54:03,700 Train:  22500 sentences\n",
      "2024-12-05 10:54:03,700         (train_with_dev=False, train_with_test=False)\n",
      "2024-12-05 10:54:03,701 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 10:54:03,701 Training Params:\n",
      "2024-12-05 10:54:03,702  - learning_rate: \"5e-05\" \n",
      "2024-12-05 10:54:03,702  - mini_batch_size: \"8\"\n",
      "2024-12-05 10:54:03,703  - max_epochs: \"5\"\n",
      "2024-12-05 10:54:03,704  - shuffle: \"True\"\n",
      "2024-12-05 10:54:03,704 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 10:54:03,705 Plugins:\n",
      "2024-12-05 10:54:03,705  - LinearScheduler | warmup_fraction: '0.1'\n",
      "2024-12-05 10:54:03,705 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 10:54:03,706 Final evaluation on model from best epoch (best-model.pt)\n",
      "2024-12-05 10:54:03,706  - metric: \"('micro avg', 'f1-score')\"\n",
      "2024-12-05 10:54:03,706 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 10:54:03,707 Computation:\n",
      "2024-12-05 10:54:03,707  - compute on device: cuda\n",
      "2024-12-05 10:54:03,708  - embedding storage: gpu\n",
      "2024-12-05 10:54:03,708 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 10:54:03,708 Model training base path: \"flair_roberta_transformer_model\"\n",
      "2024-12-05 10:54:03,709 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 10:54:03,709 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cheng\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:370: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 10:54:52,777 epoch 1 - iter 281/2813 - loss 0.55673593 - time (sec): 49.07 - samples/sec: 45.82 - lr: 0.000010 - momentum: 0.000000\n",
      "2024-12-05 10:55:40,834 epoch 1 - iter 562/2813 - loss 0.48512997 - time (sec): 97.12 - samples/sec: 46.29 - lr: 0.000020 - momentum: 0.000000\n",
      "2024-12-05 10:56:28,486 epoch 1 - iter 843/2813 - loss 0.46255406 - time (sec): 144.78 - samples/sec: 46.58 - lr: 0.000030 - momentum: 0.000000\n",
      "2024-12-05 10:57:16,357 epoch 1 - iter 1124/2813 - loss 0.44995912 - time (sec): 192.65 - samples/sec: 46.68 - lr: 0.000040 - momentum: 0.000000\n",
      "2024-12-05 10:58:04,398 epoch 1 - iter 1405/2813 - loss 0.43937684 - time (sec): 240.69 - samples/sec: 46.70 - lr: 0.000050 - momentum: 0.000000\n",
      "2024-12-05 10:58:52,167 epoch 1 - iter 1686/2813 - loss 0.43399941 - time (sec): 288.46 - samples/sec: 46.76 - lr: 0.000049 - momentum: 0.000000\n",
      "2024-12-05 10:59:39,828 epoch 1 - iter 1967/2813 - loss 0.43466675 - time (sec): 336.12 - samples/sec: 46.82 - lr: 0.000048 - momentum: 0.000000\n",
      "2024-12-05 11:00:27,137 epoch 1 - iter 2248/2813 - loss 0.43073257 - time (sec): 383.43 - samples/sec: 46.90 - lr: 0.000047 - momentum: 0.000000\n",
      "2024-12-05 11:01:14,019 epoch 1 - iter 2529/2813 - loss 0.42344380 - time (sec): 430.31 - samples/sec: 47.02 - lr: 0.000046 - momentum: 0.000000\n",
      "2024-12-05 11:02:00,714 epoch 1 - iter 2810/2813 - loss 0.41470456 - time (sec): 477.00 - samples/sec: 47.13 - lr: 0.000044 - momentum: 0.000000\n",
      "2024-12-05 11:02:01,196 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 11:02:01,196 EPOCH 1 done: loss 0.4147 - lr: 0.000044\n",
      "2024-12-05 11:02:01,197 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:25<00:00,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 11:02:27,397 DEV : loss 0.257555216550827 - f1-score (micro avg)  0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 11:02:33,898 saving best model\n",
      "2024-12-05 11:02:35,092 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 11:03:23,040 epoch 2 - iter 281/2813 - loss 0.30794823 - time (sec): 47.95 - samples/sec: 46.88 - lr: 0.000043 - momentum: 0.000000\n",
      "2024-12-05 11:04:10,863 epoch 2 - iter 562/2813 - loss 0.28643736 - time (sec): 95.77 - samples/sec: 46.95 - lr: 0.000042 - momentum: 0.000000\n",
      "2024-12-05 11:04:58,139 epoch 2 - iter 843/2813 - loss 0.28425419 - time (sec): 143.05 - samples/sec: 47.15 - lr: 0.000041 - momentum: 0.000000\n",
      "2024-12-05 11:05:45,386 epoch 2 - iter 1124/2813 - loss 0.26957155 - time (sec): 190.29 - samples/sec: 47.25 - lr: 0.000040 - momentum: 0.000000\n",
      "2024-12-05 11:06:32,702 epoch 2 - iter 1405/2813 - loss 0.27634417 - time (sec): 237.61 - samples/sec: 47.30 - lr: 0.000039 - momentum: 0.000000\n",
      "2024-12-05 11:07:19,994 epoch 2 - iter 1686/2813 - loss 0.27581874 - time (sec): 284.90 - samples/sec: 47.34 - lr: 0.000038 - momentum: 0.000000\n",
      "2024-12-05 11:08:06,705 epoch 2 - iter 1967/2813 - loss 0.27587281 - time (sec): 331.61 - samples/sec: 47.45 - lr: 0.000037 - momentum: 0.000000\n",
      "2024-12-05 11:08:53,487 epoch 2 - iter 2248/2813 - loss 0.27391171 - time (sec): 378.39 - samples/sec: 47.53 - lr: 0.000036 - momentum: 0.000000\n",
      "2024-12-05 11:09:41,244 epoch 2 - iter 2529/2813 - loss 0.27387381 - time (sec): 426.15 - samples/sec: 47.48 - lr: 0.000034 - momentum: 0.000000\n",
      "2024-12-05 11:10:28,782 epoch 2 - iter 2810/2813 - loss 0.27029301 - time (sec): 473.69 - samples/sec: 47.46 - lr: 0.000033 - momentum: 0.000000\n",
      "2024-12-05 11:10:29,265 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 11:10:29,266 EPOCH 2 done: loss 0.2710 - lr: 0.000033\n",
      "2024-12-05 11:10:29,266 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:25<00:00,  6.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 11:10:55,868 DEV : loss 0.46414676308631897 - f1-score (micro avg)  0.918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 11:11:02,823 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 11:11:50,782 epoch 3 - iter 281/2813 - loss 0.16105546 - time (sec): 47.96 - samples/sec: 46.88 - lr: 0.000032 - momentum: 0.000000\n",
      "2024-12-05 11:12:38,259 epoch 3 - iter 562/2813 - loss 0.16029461 - time (sec): 95.43 - samples/sec: 47.11 - lr: 0.000031 - momentum: 0.000000\n",
      "2024-12-05 11:13:25,502 epoch 3 - iter 843/2813 - loss 0.16207369 - time (sec): 142.68 - samples/sec: 47.27 - lr: 0.000030 - momentum: 0.000000\n",
      "2024-12-05 11:14:13,163 epoch 3 - iter 1124/2813 - loss 0.15758802 - time (sec): 190.34 - samples/sec: 47.24 - lr: 0.000029 - momentum: 0.000000\n",
      "2024-12-05 11:15:00,483 epoch 3 - iter 1405/2813 - loss 0.15653109 - time (sec): 237.66 - samples/sec: 47.29 - lr: 0.000028 - momentum: 0.000000\n",
      "2024-12-05 11:15:48,244 epoch 3 - iter 1686/2813 - loss 0.16046691 - time (sec): 285.42 - samples/sec: 47.26 - lr: 0.000027 - momentum: 0.000000\n",
      "2024-12-05 11:16:35,492 epoch 3 - iter 1967/2813 - loss 0.16385196 - time (sec): 332.67 - samples/sec: 47.30 - lr: 0.000026 - momentum: 0.000000\n",
      "2024-12-05 11:17:22,183 epoch 3 - iter 2248/2813 - loss 0.16684310 - time (sec): 379.36 - samples/sec: 47.41 - lr: 0.000024 - momentum: 0.000000\n",
      "2024-12-05 11:18:09,501 epoch 3 - iter 2529/2813 - loss 0.16544374 - time (sec): 426.68 - samples/sec: 47.42 - lr: 0.000023 - momentum: 0.000000\n",
      "2024-12-05 11:18:57,214 epoch 3 - iter 2810/2813 - loss 0.16704456 - time (sec): 474.39 - samples/sec: 47.39 - lr: 0.000022 - momentum: 0.000000\n",
      "2024-12-05 11:18:57,645 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 11:18:57,645 EPOCH 3 done: loss 0.1669 - lr: 0.000022\n",
      "2024-12-05 11:18:57,646 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:26<00:00,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 11:19:24,708 DEV : loss 0.4041275084018707 - f1-score (micro avg)  0.9308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 11:19:31,375 saving best model\n",
      "2024-12-05 11:19:32,480 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 11:20:20,812 epoch 4 - iter 281/2813 - loss 0.08424391 - time (sec): 48.33 - samples/sec: 46.51 - lr: 0.000021 - momentum: 0.000000\n",
      "2024-12-05 11:21:08,211 epoch 4 - iter 562/2813 - loss 0.07993533 - time (sec): 95.73 - samples/sec: 46.97 - lr: 0.000020 - momentum: 0.000000\n",
      "2024-12-05 11:21:54,955 epoch 4 - iter 843/2813 - loss 0.07658078 - time (sec): 142.47 - samples/sec: 47.34 - lr: 0.000019 - momentum: 0.000000\n",
      "2024-12-05 11:22:42,856 epoch 4 - iter 1124/2813 - loss 0.08071680 - time (sec): 190.37 - samples/sec: 47.23 - lr: 0.000018 - momentum: 0.000000\n",
      "2024-12-05 11:23:30,963 epoch 4 - iter 1405/2813 - loss 0.08696325 - time (sec): 238.48 - samples/sec: 47.13 - lr: 0.000017 - momentum: 0.000000\n",
      "2024-12-05 11:24:19,891 epoch 4 - iter 1686/2813 - loss 0.08438743 - time (sec): 287.41 - samples/sec: 46.93 - lr: 0.000016 - momentum: 0.000000\n",
      "2024-12-05 11:25:07,531 epoch 4 - iter 1967/2813 - loss 0.08522017 - time (sec): 335.05 - samples/sec: 46.97 - lr: 0.000014 - momentum: 0.000000\n",
      "2024-12-05 11:25:55,131 epoch 4 - iter 2248/2813 - loss 0.08285793 - time (sec): 382.65 - samples/sec: 47.00 - lr: 0.000013 - momentum: 0.000000\n",
      "2024-12-05 11:26:42,211 epoch 4 - iter 2529/2813 - loss 0.08342293 - time (sec): 429.73 - samples/sec: 47.08 - lr: 0.000012 - momentum: 0.000000\n",
      "2024-12-05 11:27:29,815 epoch 4 - iter 2810/2813 - loss 0.08194138 - time (sec): 477.33 - samples/sec: 47.09 - lr: 0.000011 - momentum: 0.000000\n",
      "2024-12-05 11:27:30,295 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 11:27:30,296 EPOCH 4 done: loss 0.0822 - lr: 0.000011\n",
      "2024-12-05 11:27:30,296 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:25<00:00,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 11:27:56,410 DEV : loss 0.3604690432548523 - f1-score (micro avg)  0.942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 11:28:03,112 saving best model\n",
      "2024-12-05 11:28:04,161 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 11:28:51,516 epoch 5 - iter 281/2813 - loss 0.03934556 - time (sec): 47.35 - samples/sec: 47.47 - lr: 0.000010 - momentum: 0.000000\n",
      "2024-12-05 11:29:38,058 epoch 5 - iter 562/2813 - loss 0.04708834 - time (sec): 93.89 - samples/sec: 47.88 - lr: 0.000009 - momentum: 0.000000\n",
      "2024-12-05 11:30:24,683 epoch 5 - iter 843/2813 - loss 0.05039754 - time (sec): 140.52 - samples/sec: 47.99 - lr: 0.000008 - momentum: 0.000000\n",
      "2024-12-05 11:31:12,197 epoch 5 - iter 1124/2813 - loss 0.04746460 - time (sec): 188.03 - samples/sec: 47.82 - lr: 0.000007 - momentum: 0.000000\n",
      "2024-12-05 11:32:00,227 epoch 5 - iter 1405/2813 - loss 0.04636933 - time (sec): 236.06 - samples/sec: 47.61 - lr: 0.000006 - momentum: 0.000000\n",
      "2024-12-05 11:32:47,304 epoch 5 - iter 1686/2813 - loss 0.04505450 - time (sec): 283.14 - samples/sec: 47.64 - lr: 0.000004 - momentum: 0.000000\n",
      "2024-12-05 11:33:35,100 epoch 5 - iter 1967/2813 - loss 0.04330481 - time (sec): 330.94 - samples/sec: 47.55 - lr: 0.000003 - momentum: 0.000000\n",
      "2024-12-05 11:34:22,270 epoch 5 - iter 2248/2813 - loss 0.04228629 - time (sec): 378.11 - samples/sec: 47.56 - lr: 0.000002 - momentum: 0.000000\n",
      "2024-12-05 11:35:10,248 epoch 5 - iter 2529/2813 - loss 0.04262095 - time (sec): 426.08 - samples/sec: 47.48 - lr: 0.000001 - momentum: 0.000000\n",
      "2024-12-05 11:35:57,892 epoch 5 - iter 2810/2813 - loss 0.04131189 - time (sec): 473.73 - samples/sec: 47.45 - lr: 0.000000 - momentum: 0.000000\n",
      "2024-12-05 11:35:58,366 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 11:35:58,366 EPOCH 5 done: loss 0.0413 - lr: 0.000000\n",
      "2024-12-05 11:35:58,367 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:25<00:00,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 11:36:24,884 DEV : loss 0.4084957540035248 - f1-score (micro avg)  0.9412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 11:36:32,431 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 11:36:32,433 Loading model from best epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [04:15<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 11:40:51,445 \n",
      "Results:\n",
      "- F-score (micro) 0.9429\n",
      "- F-score (macro) 0.9429\n",
      "- Accuracy 0.9429\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         POS     0.9317    0.9559    0.9437     12500\n",
      "         NEG     0.9547    0.9299    0.9422     12500\n",
      "\n",
      "    accuracy                         0.9429     25000\n",
      "   macro avg     0.9432    0.9429    0.9429     25000\n",
      "weighted avg     0.9432    0.9429    0.9429     25000\n",
      "\n",
      "2024-12-05 11:40:51,446 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.94292}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the logging level to INFO\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = ModelTrainer(transformer_classifier, corpus)\n",
    "\n",
    "#Fine-tune the model\n",
    "trainer.fine_tune(\n",
    "    base_path='flair_roberta_transformer_model',             # Directory to save the model and logs\n",
    "    learning_rate=5e-5,                  # Learning rate for fine-tuning\n",
    "    mini_batch_size=8,                   # Smaller batch size for transformers\n",
    "    max_epochs=5,                        # Number of epochs\n",
    "    embeddings_storage_mode='gpu',     \n",
    "    optimizer=AdamW,                     # Optimizer suited for transformers\n",
    "    save_final_model=True,               # Save the final model\n",
    "    save_model_each_k_epochs=1,          # Save model checkpoint every epoch\n",
    "    create_file_logs=True,               # Save logs to a file\n",
    "    create_loss_file=True,               # Save loss values to a file\n",
    "    use_final_model_for_eval=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 11:43:51,397 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 11:43:51,398 Model: \"TextClassifier(\n",
      "  (embeddings): TransformerDocumentEmbeddings(\n",
      "    (model): RobertaModel(\n",
      "      (embeddings): RobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(50266, 768, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-11): 12 x RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSdpaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): RobertaPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (locked_dropout): LockedDropout(p=0.0)\n",
      "  (word_dropout): WordDropout(p=0.0)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2024-12-05 11:43:51,398 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 11:43:51,398 Corpus: 22500 train + 2500 dev + 25000 test sentences\n",
      "2024-12-05 11:43:51,399 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 11:43:51,399 Train:  22500 sentences\n",
      "2024-12-05 11:43:51,400         (train_with_dev=False, train_with_test=False)\n",
      "2024-12-05 11:43:51,400 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 11:43:51,400 Training Params:\n",
      "2024-12-05 11:43:51,401  - learning_rate: \"5e-05\" \n",
      "2024-12-05 11:43:51,401  - mini_batch_size: \"8\"\n",
      "2024-12-05 11:43:51,401  - max_epochs: \"10\"\n",
      "2024-12-05 11:43:51,402  - shuffle: \"True\"\n",
      "2024-12-05 11:43:51,402 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 11:43:51,402 Plugins:\n",
      "2024-12-05 11:43:51,403  - LinearScheduler | warmup_fraction: '0.1'\n",
      "2024-12-05 11:43:51,404 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 11:43:51,404 Final evaluation on model from best epoch (best-model.pt)\n",
      "2024-12-05 11:43:51,404  - metric: \"('micro avg', 'f1-score')\"\n",
      "2024-12-05 11:43:51,405 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 11:43:51,405 Computation:\n",
      "2024-12-05 11:43:51,406  - compute on device: cuda\n",
      "2024-12-05 11:43:51,406  - embedding storage: gpu\n",
      "2024-12-05 11:43:51,407 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 11:43:51,407 Model training base path: \"flair_roberta_transformer_10_model\"\n",
      "2024-12-05 11:43:51,408 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 11:43:51,408 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cheng\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:370: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 11:44:39,316 epoch 1 - iter 281/2813 - loss 0.58373917 - time (sec): 47.91 - samples/sec: 46.92 - lr: 0.000005 - momentum: 0.000000\n",
      "2024-12-05 11:45:27,015 epoch 1 - iter 562/2813 - loss 0.47220784 - time (sec): 95.61 - samples/sec: 47.03 - lr: 0.000010 - momentum: 0.000000\n",
      "2024-12-05 11:46:14,462 epoch 1 - iter 843/2813 - loss 0.44223183 - time (sec): 143.05 - samples/sec: 47.14 - lr: 0.000015 - momentum: 0.000000\n",
      "2024-12-05 11:47:02,329 epoch 1 - iter 1124/2813 - loss 0.42621069 - time (sec): 190.92 - samples/sec: 47.10 - lr: 0.000020 - momentum: 0.000000\n",
      "2024-12-05 11:47:50,144 epoch 1 - iter 1405/2813 - loss 0.41301586 - time (sec): 238.74 - samples/sec: 47.08 - lr: 0.000025 - momentum: 0.000000\n",
      "2024-12-05 11:48:38,050 epoch 1 - iter 1686/2813 - loss 0.40821100 - time (sec): 286.64 - samples/sec: 47.06 - lr: 0.000030 - momentum: 0.000000\n",
      "2024-12-05 11:49:25,752 epoch 1 - iter 1967/2813 - loss 0.40049963 - time (sec): 334.34 - samples/sec: 47.07 - lr: 0.000035 - momentum: 0.000000\n",
      "2024-12-05 11:50:13,396 epoch 1 - iter 2248/2813 - loss 0.39816368 - time (sec): 381.99 - samples/sec: 47.08 - lr: 0.000040 - momentum: 0.000000\n",
      "2024-12-05 11:51:00,962 epoch 1 - iter 2529/2813 - loss 0.39883283 - time (sec): 429.55 - samples/sec: 47.10 - lr: 0.000045 - momentum: 0.000000\n",
      "2024-12-05 11:51:48,371 epoch 1 - iter 2810/2813 - loss 0.39749746 - time (sec): 476.96 - samples/sec: 47.13 - lr: 0.000050 - momentum: 0.000000\n",
      "2024-12-05 11:51:48,874 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 11:51:48,875 EPOCH 1 done: loss 0.3974 - lr: 0.000050\n",
      "2024-12-05 11:51:48,875 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:26<00:00,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 11:52:15,965 DEV : loss 0.7565402388572693 - f1-score (micro avg)  0.8396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 11:52:23,490 saving best model\n",
      "2024-12-05 11:52:24,363 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 11:53:13,511 epoch 2 - iter 281/2813 - loss 0.32835130 - time (sec): 49.15 - samples/sec: 45.74 - lr: 0.000049 - momentum: 0.000000\n",
      "2024-12-05 11:54:01,231 epoch 2 - iter 562/2813 - loss 0.32176834 - time (sec): 96.87 - samples/sec: 46.41 - lr: 0.000049 - momentum: 0.000000\n",
      "2024-12-05 11:54:48,502 epoch 2 - iter 843/2813 - loss 0.33887666 - time (sec): 144.14 - samples/sec: 46.79 - lr: 0.000048 - momentum: 0.000000\n",
      "2024-12-05 11:55:36,311 epoch 2 - iter 1124/2813 - loss 0.33466865 - time (sec): 191.95 - samples/sec: 46.85 - lr: 0.000048 - momentum: 0.000000\n",
      "2024-12-05 11:56:22,962 epoch 2 - iter 1405/2813 - loss 0.33086620 - time (sec): 238.60 - samples/sec: 47.11 - lr: 0.000047 - momentum: 0.000000\n",
      "2024-12-05 11:57:09,412 epoch 2 - iter 1686/2813 - loss 0.31990122 - time (sec): 285.05 - samples/sec: 47.32 - lr: 0.000047 - momentum: 0.000000\n",
      "2024-12-05 11:57:56,823 epoch 2 - iter 1967/2813 - loss 0.31419163 - time (sec): 332.46 - samples/sec: 47.33 - lr: 0.000046 - momentum: 0.000000\n",
      "2024-12-05 11:58:43,151 epoch 2 - iter 2248/2813 - loss 0.30956071 - time (sec): 378.79 - samples/sec: 47.48 - lr: 0.000046 - momentum: 0.000000\n",
      "2024-12-05 11:59:29,644 epoch 2 - iter 2529/2813 - loss 0.31069818 - time (sec): 425.28 - samples/sec: 47.57 - lr: 0.000045 - momentum: 0.000000\n",
      "2024-12-05 12:00:16,405 epoch 2 - iter 2810/2813 - loss 0.31079219 - time (sec): 472.04 - samples/sec: 47.62 - lr: 0.000044 - momentum: 0.000000\n",
      "2024-12-05 12:00:16,902 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 12:00:16,902 EPOCH 2 done: loss 0.3105 - lr: 0.000044\n",
      "2024-12-05 12:00:16,902 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:25<00:00,  6.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 12:00:42,742 DEV : loss 0.32830020785331726 - f1-score (micro avg)  0.9332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 12:00:49,260 saving best model\n",
      "2024-12-05 12:00:49,717 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 12:01:37,252 epoch 3 - iter 281/2813 - loss 0.18962833 - time (sec): 47.53 - samples/sec: 47.29 - lr: 0.000044 - momentum: 0.000000\n",
      "2024-12-05 12:02:24,812 epoch 3 - iter 562/2813 - loss 0.20125464 - time (sec): 95.09 - samples/sec: 47.28 - lr: 0.000043 - momentum: 0.000000\n",
      "2024-12-05 12:03:12,134 epoch 3 - iter 843/2813 - loss 0.20365776 - time (sec): 142.41 - samples/sec: 47.35 - lr: 0.000043 - momentum: 0.000000\n",
      "2024-12-05 12:04:00,423 epoch 3 - iter 1124/2813 - loss 0.21156227 - time (sec): 190.70 - samples/sec: 47.15 - lr: 0.000042 - momentum: 0.000000\n",
      "2024-12-05 12:04:46,660 epoch 3 - iter 1405/2813 - loss 0.21561870 - time (sec): 236.94 - samples/sec: 47.44 - lr: 0.000042 - momentum: 0.000000\n",
      "2024-12-05 12:05:33,888 epoch 3 - iter 1686/2813 - loss 0.21712826 - time (sec): 284.17 - samples/sec: 47.46 - lr: 0.000041 - momentum: 0.000000\n",
      "2024-12-05 12:06:20,439 epoch 3 - iter 1967/2813 - loss 0.22108022 - time (sec): 330.72 - samples/sec: 47.58 - lr: 0.000041 - momentum: 0.000000\n",
      "2024-12-05 12:07:07,216 epoch 3 - iter 2248/2813 - loss 0.22089093 - time (sec): 377.50 - samples/sec: 47.64 - lr: 0.000040 - momentum: 0.000000\n",
      "2024-12-05 12:07:54,263 epoch 3 - iter 2529/2813 - loss 0.22043105 - time (sec): 424.54 - samples/sec: 47.66 - lr: 0.000039 - momentum: 0.000000\n",
      "2024-12-05 12:08:41,456 epoch 3 - iter 2810/2813 - loss 0.22169059 - time (sec): 471.74 - samples/sec: 47.65 - lr: 0.000039 - momentum: 0.000000\n",
      "2024-12-05 12:08:41,962 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 12:08:41,962 EPOCH 3 done: loss 0.2219 - lr: 0.000039\n",
      "2024-12-05 12:08:41,963 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:25<00:00,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 12:09:07,952 DEV : loss 0.3040253520011902 - f1-score (micro avg)  0.9356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 12:09:14,812 saving best model\n",
      "2024-12-05 12:09:15,343 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 12:10:02,921 epoch 4 - iter 281/2813 - loss 0.12074864 - time (sec): 47.58 - samples/sec: 47.25 - lr: 0.000038 - momentum: 0.000000\n",
      "2024-12-05 12:10:50,157 epoch 4 - iter 562/2813 - loss 0.13219092 - time (sec): 94.81 - samples/sec: 47.42 - lr: 0.000038 - momentum: 0.000000\n",
      "2024-12-05 12:11:37,081 epoch 4 - iter 843/2813 - loss 0.14922086 - time (sec): 141.74 - samples/sec: 47.58 - lr: 0.000037 - momentum: 0.000000\n",
      "2024-12-05 12:12:23,143 epoch 4 - iter 1124/2813 - loss 0.16284989 - time (sec): 187.80 - samples/sec: 47.88 - lr: 0.000037 - momentum: 0.000000\n",
      "2024-12-05 12:13:09,798 epoch 4 - iter 1405/2813 - loss 0.16361103 - time (sec): 234.45 - samples/sec: 47.94 - lr: 0.000036 - momentum: 0.000000\n",
      "2024-12-05 12:13:56,641 epoch 4 - iter 1686/2813 - loss 0.17118494 - time (sec): 281.30 - samples/sec: 47.95 - lr: 0.000036 - momentum: 0.000000\n",
      "2024-12-05 12:14:42,985 epoch 4 - iter 1967/2813 - loss 0.17361791 - time (sec): 327.64 - samples/sec: 48.03 - lr: 0.000035 - momentum: 0.000000\n",
      "2024-12-05 12:15:29,801 epoch 4 - iter 2248/2813 - loss 0.17247634 - time (sec): 374.46 - samples/sec: 48.03 - lr: 0.000034 - momentum: 0.000000\n",
      "2024-12-05 12:16:17,518 epoch 4 - iter 2529/2813 - loss 0.17284306 - time (sec): 422.17 - samples/sec: 47.92 - lr: 0.000034 - momentum: 0.000000\n",
      "2024-12-05 12:17:04,093 epoch 4 - iter 2810/2813 - loss 0.17104659 - time (sec): 468.75 - samples/sec: 47.96 - lr: 0.000033 - momentum: 0.000000\n",
      "2024-12-05 12:17:04,585 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 12:17:04,586 EPOCH 4 done: loss 0.1710 - lr: 0.000033\n",
      "2024-12-05 12:17:04,586 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:25<00:00,  6.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 12:17:30,810 DEV : loss 0.4105227291584015 - f1-score (micro avg)  0.9356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 12:17:37,383 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 12:18:24,508 epoch 5 - iter 281/2813 - loss 0.10495093 - time (sec): 47.12 - samples/sec: 47.70 - lr: 0.000033 - momentum: 0.000000\n",
      "2024-12-05 12:19:11,754 epoch 5 - iter 562/2813 - loss 0.10767603 - time (sec): 94.37 - samples/sec: 47.64 - lr: 0.000032 - momentum: 0.000000\n",
      "2024-12-05 12:19:58,018 epoch 5 - iter 843/2813 - loss 0.11383192 - time (sec): 140.63 - samples/sec: 47.95 - lr: 0.000032 - momentum: 0.000000\n",
      "2024-12-05 12:20:44,566 epoch 5 - iter 1124/2813 - loss 0.11603135 - time (sec): 187.18 - samples/sec: 48.04 - lr: 0.000031 - momentum: 0.000000\n",
      "2024-12-05 12:21:30,844 epoch 5 - iter 1405/2813 - loss 0.12203325 - time (sec): 233.46 - samples/sec: 48.15 - lr: 0.000031 - momentum: 0.000000\n",
      "2024-12-05 12:22:16,933 epoch 5 - iter 1686/2813 - loss 0.12418234 - time (sec): 279.55 - samples/sec: 48.25 - lr: 0.000030 - momentum: 0.000000\n",
      "2024-12-05 12:23:02,891 epoch 5 - iter 1967/2813 - loss 0.12646547 - time (sec): 325.51 - samples/sec: 48.34 - lr: 0.000029 - momentum: 0.000000\n",
      "2024-12-05 12:23:49,099 epoch 5 - iter 2248/2813 - loss 0.12738111 - time (sec): 371.71 - samples/sec: 48.38 - lr: 0.000029 - momentum: 0.000000\n",
      "2024-12-05 12:24:35,631 epoch 5 - iter 2529/2813 - loss 0.12890253 - time (sec): 418.25 - samples/sec: 48.37 - lr: 0.000028 - momentum: 0.000000\n",
      "2024-12-05 12:25:22,476 epoch 5 - iter 2810/2813 - loss 0.12963627 - time (sec): 465.09 - samples/sec: 48.33 - lr: 0.000028 - momentum: 0.000000\n",
      "2024-12-05 12:25:22,929 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 12:25:22,930 EPOCH 5 done: loss 0.1295 - lr: 0.000028\n",
      "2024-12-05 12:25:22,930 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:26<00:00,  6.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 12:25:49,475 DEV : loss 0.35577234625816345 - f1-score (micro avg)  0.9376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 12:25:56,216 saving best model\n",
      "2024-12-05 12:25:56,727 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 12:26:43,379 epoch 6 - iter 281/2813 - loss 0.06666695 - time (sec): 46.65 - samples/sec: 48.19 - lr: 0.000027 - momentum: 0.000000\n",
      "2024-12-05 12:27:30,278 epoch 6 - iter 562/2813 - loss 0.07470844 - time (sec): 93.55 - samples/sec: 48.06 - lr: 0.000027 - momentum: 0.000000\n",
      "2024-12-05 12:28:16,821 epoch 6 - iter 843/2813 - loss 0.07611373 - time (sec): 140.09 - samples/sec: 48.14 - lr: 0.000026 - momentum: 0.000000\n",
      "2024-12-05 12:29:04,165 epoch 6 - iter 1124/2813 - loss 0.08337160 - time (sec): 187.44 - samples/sec: 47.97 - lr: 0.000026 - momentum: 0.000000\n",
      "2024-12-05 12:29:51,207 epoch 6 - iter 1405/2813 - loss 0.08263908 - time (sec): 234.48 - samples/sec: 47.94 - lr: 0.000025 - momentum: 0.000000\n",
      "2024-12-05 12:30:37,847 epoch 6 - iter 1686/2813 - loss 0.08722331 - time (sec): 281.12 - samples/sec: 47.98 - lr: 0.000024 - momentum: 0.000000\n",
      "2024-12-05 12:31:24,916 epoch 6 - iter 1967/2813 - loss 0.08996228 - time (sec): 328.19 - samples/sec: 47.95 - lr: 0.000024 - momentum: 0.000000\n",
      "2024-12-05 12:32:11,877 epoch 6 - iter 2248/2813 - loss 0.09126646 - time (sec): 375.15 - samples/sec: 47.94 - lr: 0.000023 - momentum: 0.000000\n",
      "2024-12-05 12:32:57,880 epoch 6 - iter 2529/2813 - loss 0.09231401 - time (sec): 421.15 - samples/sec: 48.04 - lr: 0.000023 - momentum: 0.000000\n",
      "2024-12-05 12:33:43,477 epoch 6 - iter 2810/2813 - loss 0.09368157 - time (sec): 466.75 - samples/sec: 48.16 - lr: 0.000022 - momentum: 0.000000\n",
      "2024-12-05 12:33:43,927 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 12:33:43,927 EPOCH 6 done: loss 0.0936 - lr: 0.000022\n",
      "2024-12-05 12:33:43,928 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:25<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 12:34:09,861 DEV : loss 0.38221585750579834 - f1-score (micro avg)  0.9376\n",
      "2024-12-05 12:34:16,741 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 12:35:02,455 epoch 7 - iter 281/2813 - loss 0.07888148 - time (sec): 45.71 - samples/sec: 49.18 - lr: 0.000022 - momentum: 0.000000\n",
      "2024-12-05 12:35:48,471 epoch 7 - iter 562/2813 - loss 0.06912298 - time (sec): 91.73 - samples/sec: 49.01 - lr: 0.000021 - momentum: 0.000000\n",
      "2024-12-05 12:36:34,191 epoch 7 - iter 843/2813 - loss 0.07083683 - time (sec): 137.45 - samples/sec: 49.07 - lr: 0.000021 - momentum: 0.000000\n",
      "2024-12-05 12:37:20,014 epoch 7 - iter 1124/2813 - loss 0.06223230 - time (sec): 183.27 - samples/sec: 49.06 - lr: 0.000020 - momentum: 0.000000\n",
      "2024-12-05 12:38:05,926 epoch 7 - iter 1405/2813 - loss 0.06322601 - time (sec): 229.18 - samples/sec: 49.04 - lr: 0.000019 - momentum: 0.000000\n",
      "2024-12-05 12:38:52,003 epoch 7 - iter 1686/2813 - loss 0.06497332 - time (sec): 275.26 - samples/sec: 49.00 - lr: 0.000019 - momentum: 0.000000\n",
      "2024-12-05 12:39:38,066 epoch 7 - iter 1967/2813 - loss 0.06368890 - time (sec): 321.32 - samples/sec: 48.97 - lr: 0.000018 - momentum: 0.000000\n",
      "2024-12-05 12:40:23,769 epoch 7 - iter 2248/2813 - loss 0.06281196 - time (sec): 367.03 - samples/sec: 49.00 - lr: 0.000018 - momentum: 0.000000\n",
      "2024-12-05 12:41:10,403 epoch 7 - iter 2529/2813 - loss 0.06027024 - time (sec): 413.66 - samples/sec: 48.91 - lr: 0.000017 - momentum: 0.000000\n",
      "2024-12-05 12:41:57,162 epoch 7 - iter 2810/2813 - loss 0.06125637 - time (sec): 460.42 - samples/sec: 48.82 - lr: 0.000017 - momentum: 0.000000\n",
      "2024-12-05 12:41:57,647 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 12:41:57,647 EPOCH 7 done: loss 0.0612 - lr: 0.000017\n",
      "2024-12-05 12:41:57,648 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:25<00:00,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 12:42:23,696 DEV : loss 0.37941882014274597 - f1-score (micro avg)  0.9436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 12:42:30,323 saving best model\n",
      "2024-12-05 12:42:30,826 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 12:43:18,447 epoch 8 - iter 281/2813 - loss 0.04706603 - time (sec): 47.62 - samples/sec: 47.21 - lr: 0.000016 - momentum: 0.000000\n",
      "2024-12-05 12:44:04,907 epoch 8 - iter 562/2813 - loss 0.04148793 - time (sec): 94.08 - samples/sec: 47.79 - lr: 0.000016 - momentum: 0.000000\n",
      "2024-12-05 12:44:50,845 epoch 8 - iter 843/2813 - loss 0.03838554 - time (sec): 140.02 - samples/sec: 48.17 - lr: 0.000015 - momentum: 0.000000\n",
      "2024-12-05 12:45:35,972 epoch 8 - iter 1124/2813 - loss 0.03788182 - time (sec): 185.14 - samples/sec: 48.57 - lr: 0.000014 - momentum: 0.000000\n",
      "2024-12-05 12:46:22,105 epoch 8 - iter 1405/2813 - loss 0.04548763 - time (sec): 231.28 - samples/sec: 48.60 - lr: 0.000014 - momentum: 0.000000\n",
      "2024-12-05 12:47:08,158 epoch 8 - iter 1686/2813 - loss 0.04615122 - time (sec): 277.33 - samples/sec: 48.64 - lr: 0.000013 - momentum: 0.000000\n",
      "2024-12-05 12:47:53,884 epoch 8 - iter 1967/2813 - loss 0.04393819 - time (sec): 323.06 - samples/sec: 48.71 - lr: 0.000013 - momentum: 0.000000\n",
      "2024-12-05 12:48:39,807 epoch 8 - iter 2248/2813 - loss 0.04254975 - time (sec): 368.98 - samples/sec: 48.74 - lr: 0.000012 - momentum: 0.000000\n",
      "2024-12-05 12:49:25,923 epoch 8 - iter 2529/2813 - loss 0.04228491 - time (sec): 415.10 - samples/sec: 48.74 - lr: 0.000012 - momentum: 0.000000\n",
      "2024-12-05 12:50:11,761 epoch 8 - iter 2810/2813 - loss 0.04242669 - time (sec): 460.93 - samples/sec: 48.77 - lr: 0.000011 - momentum: 0.000000\n",
      "2024-12-05 12:50:12,239 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 12:50:12,240 EPOCH 8 done: loss 0.0424 - lr: 0.000011\n",
      "2024-12-05 12:50:12,240 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:25<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 12:50:38,157 DEV : loss 0.42150649428367615 - f1-score (micro avg)  0.9412\n",
      "2024-12-05 12:50:44,746 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 12:51:31,240 epoch 9 - iter 281/2813 - loss 0.03191176 - time (sec): 46.49 - samples/sec: 48.35 - lr: 0.000011 - momentum: 0.000000\n",
      "2024-12-05 12:52:17,132 epoch 9 - iter 562/2813 - loss 0.02966251 - time (sec): 92.38 - samples/sec: 48.67 - lr: 0.000010 - momentum: 0.000000\n",
      "2024-12-05 12:53:02,815 epoch 9 - iter 843/2813 - loss 0.03044990 - time (sec): 138.07 - samples/sec: 48.85 - lr: 0.000009 - momentum: 0.000000\n",
      "2024-12-05 12:53:48,809 epoch 9 - iter 1124/2813 - loss 0.03087598 - time (sec): 184.06 - samples/sec: 48.85 - lr: 0.000009 - momentum: 0.000000\n",
      "2024-12-05 12:54:35,002 epoch 9 - iter 1405/2813 - loss 0.02867330 - time (sec): 230.25 - samples/sec: 48.82 - lr: 0.000008 - momentum: 0.000000\n",
      "2024-12-05 12:55:20,682 epoch 9 - iter 1686/2813 - loss 0.02895149 - time (sec): 275.93 - samples/sec: 48.88 - lr: 0.000008 - momentum: 0.000000\n",
      "2024-12-05 12:56:06,678 epoch 9 - iter 1967/2813 - loss 0.03050913 - time (sec): 321.93 - samples/sec: 48.88 - lr: 0.000007 - momentum: 0.000000\n",
      "2024-12-05 12:56:52,862 epoch 9 - iter 2248/2813 - loss 0.02928093 - time (sec): 368.11 - samples/sec: 48.85 - lr: 0.000007 - momentum: 0.000000\n",
      "2024-12-05 12:57:39,025 epoch 9 - iter 2529/2813 - loss 0.02903606 - time (sec): 414.28 - samples/sec: 48.84 - lr: 0.000006 - momentum: 0.000000\n",
      "2024-12-05 12:58:25,237 epoch 9 - iter 2810/2813 - loss 0.02969997 - time (sec): 460.49 - samples/sec: 48.82 - lr: 0.000006 - momentum: 0.000000\n",
      "2024-12-05 12:58:25,715 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 12:58:25,716 EPOCH 9 done: loss 0.0297 - lr: 0.000006\n",
      "2024-12-05 12:58:25,716 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:25<00:00,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 12:58:51,635 DEV : loss 0.42656537890434265 - f1-score (micro avg)  0.9448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 12:58:58,271 saving best model\n",
      "2024-12-05 12:58:58,832 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 12:59:44,968 epoch 10 - iter 281/2813 - loss 0.01800937 - time (sec): 46.13 - samples/sec: 48.73 - lr: 0.000005 - momentum: 0.000000\n",
      "2024-12-05 13:00:31,129 epoch 10 - iter 562/2813 - loss 0.01297621 - time (sec): 92.30 - samples/sec: 48.71 - lr: 0.000004 - momentum: 0.000000\n",
      "2024-12-05 13:01:17,366 epoch 10 - iter 843/2813 - loss 0.01387568 - time (sec): 138.53 - samples/sec: 48.68 - lr: 0.000004 - momentum: 0.000000\n",
      "2024-12-05 13:02:03,700 epoch 10 - iter 1124/2813 - loss 0.01678401 - time (sec): 184.87 - samples/sec: 48.64 - lr: 0.000003 - momentum: 0.000000\n",
      "2024-12-05 13:02:49,472 epoch 10 - iter 1405/2813 - loss 0.01468636 - time (sec): 230.64 - samples/sec: 48.73 - lr: 0.000003 - momentum: 0.000000\n",
      "2024-12-05 13:03:35,408 epoch 10 - iter 1686/2813 - loss 0.01589973 - time (sec): 276.57 - samples/sec: 48.77 - lr: 0.000002 - momentum: 0.000000\n",
      "2024-12-05 13:04:21,498 epoch 10 - iter 1967/2813 - loss 0.01644449 - time (sec): 322.66 - samples/sec: 48.77 - lr: 0.000002 - momentum: 0.000000\n",
      "2024-12-05 13:05:08,201 epoch 10 - iter 2248/2813 - loss 0.01663039 - time (sec): 369.37 - samples/sec: 48.69 - lr: 0.000001 - momentum: 0.000000\n",
      "2024-12-05 13:05:54,205 epoch 10 - iter 2529/2813 - loss 0.01708956 - time (sec): 415.37 - samples/sec: 48.71 - lr: 0.000001 - momentum: 0.000000\n",
      "2024-12-05 13:06:40,787 epoch 10 - iter 2810/2813 - loss 0.01824988 - time (sec): 461.95 - samples/sec: 48.66 - lr: 0.000000 - momentum: 0.000000\n",
      "2024-12-05 13:06:41,192 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 13:06:41,193 EPOCH 10 done: loss 0.0182 - lr: 0.000000\n",
      "2024-12-05 13:06:41,193 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:25<00:00,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 13:07:07,265 DEV : loss 0.4370265603065491 - f1-score (micro avg)  0.9456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 13:07:13,963 saving best model\n",
      "2024-12-05 13:07:14,951 ----------------------------------------------------------------------------------------------------\n",
      "2024-12-05 13:07:14,953 Loading model from best epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [04:13<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 13:11:31,168 \n",
      "Results:\n",
      "- F-score (micro) 0.9432\n",
      "- F-score (macro) 0.9432\n",
      "- Accuracy 0.9432\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         POS     0.9408    0.9460    0.9434     12500\n",
      "         NEG     0.9457    0.9405    0.9431     12500\n",
      "\n",
      "    accuracy                         0.9432     25000\n",
      "   macro avg     0.9433    0.9432    0.9432     25000\n",
      "weighted avg     0.9433    0.9432    0.9432     25000\n",
      "\n",
      "2024-12-05 13:11:31,168 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.94324}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is for the 10 epochs\n",
    "\n",
    "# Set the logging level to INFO\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = ModelTrainer(transformer_classifier, corpus)\n",
    "\n",
    "#Fine-tune the model\n",
    "trainer.fine_tune(\n",
    "    base_path='flair_roberta_transformer_10_model',             # Directory to save the model and logs\n",
    "    learning_rate=5e-5,                  # Learning rate for fine-tuning\n",
    "    mini_batch_size=8,                   # Smaller batch size for transformers\n",
    "    max_epochs=10,                        # Number of epochs\n",
    "    embeddings_storage_mode='gpu',     \n",
    "    optimizer=AdamW,                     # Optimizer suited for transformers\n",
    "    save_final_model=True,               # Save the final model\n",
    "    save_model_each_k_epochs=1,          # Save model checkpoint every epoch\n",
    "    create_file_logs=True,               # Save logs to a file\n",
    "    create_loss_file=True,               # Save loss values to a file\n",
    "    use_final_model_for_eval=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the saved model\n",
    "saved_model_path = \"flair_roberta_transformer_model/best-model.pt\"  # Update this if the path or filename is different\n",
    "\n",
    "# Load the trained model\n",
    "transformer_classifier = TextClassifier.load(saved_model_path).to(flair.device)\n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_model(transformer_classifier, corpus.test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
