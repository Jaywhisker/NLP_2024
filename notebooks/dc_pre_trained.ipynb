{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook exploring existing state of the art models\n",
    "\n",
    "We explored the scores of existing pre-trained model to set the preceding benchmark\n",
    "\n",
    "The following models were tested\n",
    "1. Textblob\n",
    "2. Vader\n",
    "3. Flair\n",
    "4. Hugging Face Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def read_imdb(data_dir, is_train):\n",
    "    \"\"\"Read the IMDb review dataset text sequences and labels.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "    data_folder = 'train' if is_train else 'test'\n",
    "    \n",
    "    for label, label_folder in enumerate(['neg', 'pos']):\n",
    "        # Retrieve full path\n",
    "        full_path = os.path.join(data_dir, data_folder, label_folder)\n",
    "        for text_file in os.listdir(full_path):\n",
    "            # Read text\n",
    "            with open(os.path.join(full_path, text_file), 'r', encoding='utf-8') as f:\n",
    "                # Add text and label\n",
    "                data.append(f.read())\n",
    "                labels.append(label)\n",
    "    \n",
    "    ### END OF YOUR CODE\n",
    "    return data, labels\n",
    "\n",
    "data_dir = \"../data/aclImdb\"\n",
    "test_data = read_imdb(data_dir, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textblob\n",
    "There is both a NaiveBayers Analyzer and Pattern Analyzer\n",
    "\n",
    "We will only be testing with PatternAnalyzer as the NaiveBayesAnalyzer is too time consuming (only relies on cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:15<00:00, 1653.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score: 0.626313288633124\n",
      "Recall Score: 0.94904\n",
      "Accuracy Score: 0.6914\n",
      "F1 Score: 0.7546197640024173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer,PatternAnalyzer\n",
    "\n",
    "# Textblob Pattern Analyzer, returns (polarity=[-1,1], subjectivity=[0,1])\n",
    "prediction_list = []\n",
    "\n",
    "for sentence, y in tqdm(zip(test_data[0], test_data[1]), total=len(test_data[0])):\n",
    "    result = TextBlob(sentence, analyzer=PatternAnalyzer()).sentiment\n",
    "    prediction = result[0]\n",
    "    if prediction >= 0:\n",
    "        prediction_list.append(1) #positive\n",
    "    else:\n",
    "        prediction_list.append(0) #negative\n",
    "\n",
    "print(\"Precision Score:\", precision_score(test_data[1], prediction_list))\n",
    "print(\"Recall Score:\", recall_score(test_data[1], prediction_list))\n",
    "print(\"Accuracy Score:\", accuracy_score(test_data[1], prediction_list))\n",
    "print(\"F1 Score:\", f1_score(test_data[1], prediction_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:59<00:00, 421.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score: 0.6479836773883821\n",
      "Recall Score: 0.86384\n",
      "Accuracy Score: 0.69728\n",
      "F1 Score: 0.7405019887532575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "prediction_list = []\n",
    "# vaderSentiment Analyzer, returns ('neg'=score, 'neu':score, 'pos'=score, 'compound':score)\n",
    "# compound is the valence score of neg, neu and pos and normalised to be within [-1,1] with -1 for negative, 1 for positive\n",
    "\n",
    "for sentence, y in tqdm(zip(test_data[0], test_data[1]), total=len(test_data[0])):\n",
    "    result = sentiment.polarity_scores(sentence)\n",
    "    prediction = result['compound']\n",
    "    if prediction >= 0:\n",
    "        prediction_list.append(1) #positive\n",
    "    else:\n",
    "        prediction_list.append(0) #negative\n",
    "\n",
    "print(\"Precision Score:\", precision_score(test_data[1], prediction_list))\n",
    "print(\"Recall Score:\", recall_score(test_data[1], prediction_list))\n",
    "print(\"Accuracy Score:\", accuracy_score(test_data[1], prediction_list))\n",
    "print(\"F1 Score:\", f1_score(test_data[1], prediction_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25000 [00:00<?, ?it/s]c:\\Users\\cheng\\Desktop\\SUTD\\Term_7\\NLP\\Final Project\\myenv\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:403: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████| 25000/25000 [03:12<00:00, 129.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score: 0.9527124773960217\n",
      "Recall Score: 0.84296\n",
      "Accuracy Score: 0.90056\n",
      "F1 Score: 0.8944821731748727\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.nn import Classifier\n",
    "\n",
    "sentiment = Classifier.load('sentiment')\n",
    "prediction_list = []\n",
    "# Flair Analyzer, returns (Sentence[4]: \"sentence\" → POSITIVE (score))\n",
    "\n",
    "for sentence, y in tqdm(zip(test_data[0], test_data[1]), total=len(test_data[0])):\n",
    "    emb_sentence = Sentence(sentence) #embed sentence\n",
    "    sentiment.predict(emb_sentence)\n",
    "    prediction = emb_sentence.labels[0].value\n",
    "    if prediction == 'POSITIVE':\n",
    "        prediction_list.append(1) #positive\n",
    "    else:\n",
    "        prediction_list.append(0) #negative\n",
    "\n",
    "print(\"Precision Score:\", precision_score(test_data[1], prediction_list))\n",
    "print(\"Recall Score:\", recall_score(test_data[1], prediction_list))\n",
    "print(\"Accuracy Score:\", accuracy_score(test_data[1], prediction_list))\n",
    "print(\"F1 Score:\", f1_score(test_data[1], prediction_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace DistilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [2:19:47<00:00,  2.98it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score: 0.9146010186757215\n",
      "Recall Score: 0.86192\n",
      "Accuracy Score: 0.89072\n",
      "F1 Score: 0.8874794069192751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "prediction_list = []\n",
    "# DistilBert, returns class where 0 is negative, 1 is positive\n",
    "\n",
    "for sentence, y in tqdm(zip(test_data[0], test_data[1]), total=len(test_data[0])):\n",
    "    emb_sentence = tokenizer(sentence,  padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\") #embed sentence, sentence length doesnt exceed 512 \n",
    "    # Disable training\n",
    "    with torch.no_grad():\n",
    "        logits = model(**emb_sentence).logits\n",
    "    # retrieve prediction\n",
    "    predicted_class = logits.argmax().item()\n",
    "    prediction_list.append(predicted_class)\n",
    "\n",
    "print(\"Precision Score:\", precision_score(test_data[1], prediction_list))\n",
    "print(\"Recall Score:\", recall_score(test_data[1], prediction_list))\n",
    "print(\"Accuracy Score:\", accuracy_score(test_data[1], prediction_list))\n",
    "print(\"F1 Score:\", f1_score(test_data[1], prediction_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [1:13:55<00:00,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score: 0.8946113495469719\n",
      "Recall Score: 0.90048\n",
      "Accuracy Score: 0.8972\n",
      "F1 Score: 0.8975360816521808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Ibrahim-Alam/finetuning-xlnet-base-cased-on-imdb\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Ibrahim-Alam/finetuning-xlnet-base-cased-on-imdb\")\n",
    "\n",
    "prediction_list = []\n",
    "# Finetuned XLNet, returns class where 0 is negative, 1 is positive\n",
    "\n",
    "for sentence, y in tqdm(zip(test_data[0], test_data[1]), total=len(test_data[0])):\n",
    "    emb_sentence = tokenizer(sentence,  padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\") #embed sentence, sentence length doesnt exceed 512 \n",
    "    # Disable training\n",
    "    with torch.no_grad():\n",
    "        logits = model(**emb_sentence).logits\n",
    "    # retrieve prediction\n",
    "    predicted_class = logits.argmax().item()\n",
    "    prediction_list.append(predicted_class)\n",
    "\n",
    "print(\"Precision Score:\", precision_score(test_data[1], prediction_list))\n",
    "print(\"Recall Score:\", recall_score(test_data[1], prediction_list))\n",
    "print(\"Accuracy Score:\", accuracy_score(test_data[1], prediction_list))\n",
    "print(\"F1 Score:\", f1_score(test_data[1], prediction_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
