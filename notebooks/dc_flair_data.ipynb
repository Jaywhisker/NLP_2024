{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiningonn/anaconda3/envs/nlp_project/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from flair.data import Corpus, Sentence\n",
    "from flair.datasets import ClassificationCorpus\n",
    "from flair.embeddings import (\n",
    "    WordEmbeddings,\n",
    "    FlairEmbeddings,\n",
    "    DocumentRNNEmbeddings,\n",
    "    TransformerWordEmbeddings,\n",
    "    StackedEmbeddings,\n",
    ")\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.training_utils import EvaluationMetric\n",
    "from flair.visual.training_curves import Plotter\n",
    "\n",
    "import flair\n",
    "flair.device = 'cpu'\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_imdb(data_dir: str, is_train: bool):\n",
    "    '''\n",
    "    Read the IMDb review dataset text sequences and labels in Flair format.\n",
    "    parameters:\n",
    "    - data_dir (str): data folder relative file path\n",
    "    - is_train (bool): when True, access train folder. else access test folder.\n",
    "    \n",
    "    return:\n",
    "    >>> reviews and assign labels each in expected format by Flair\n",
    "    '''\n",
    "    data_folder = 'train' if is_train else 'test'\n",
    "    data = []\n",
    "\n",
    "    for label_folder in ['neg', 'pos']:\n",
    "        path = os.path.join(data_dir, data_folder, label_folder)\n",
    "        label = '__label__NEG' if label_folder == 'neg' else '__label__POS'\n",
    "\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as f:\n",
    "                text = f.read().replace('\\n', ' ')  # Remove newline characters\n",
    "                data.append(f\"{label} {text}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 22500\n",
      "Number of validation samples: 2500\n",
      "Number of test samples: 25000\n",
      "__label__NEG On the surface the idea of Omen 4 was good. It's nice to see that the devil child could be a girl. In fact, sometimes, as in the Exorcist, when girls are possessed or are devilry it's very effective. But in Omen 4, it stunk.<br /><br />Delia does not make me think that she could be a devil child, rather she is a child with issues. Issues that maybe only a therapist, rather then a priest could help. She does not look scary or devilish. Rather, she looks sulky and moody.<br /><br />This film had potential and if it was made by the same people who had made the previous three films it could of worked. But it's rather insulting really to make a sequel to one of the most favoured horror trilogies, as a made for TV movie special.<br /><br />On so many levels it lets down. It's cheap looking, the acting is hammish and the effects are typical of a TV drama. The characters do not bring any sympathy, and you do not route for them. I recently re-watched it after someone brought it for me for Christmas, and it has dated appalling.<br /><br />If your thinking of watching this, then I would suggest that you don't. Watch one of the others, or watch the Exorcist, or watch The Good Son. Just don't waste your time on this drivel!\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data/aclImdb\"\n",
    "# read data\n",
    "train_data = read_imdb(data_dir, is_train=True)\n",
    "test_data = read_imdb(data_dir, is_train=False)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "train_data, dev_data = train_test_split(\n",
    "    train_data, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# print dataset sizes\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(dev_data)}\")\n",
    "print(f\"Number of test samples: {len(test_data)}\")\n",
    "\n",
    "# print first 5 sample of training samples\n",
    "print(train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the output directory exists\n",
    "flair_data_folder = \"../data/flair_data\"\n",
    "os.makedirs(flair_data_folder, exist_ok=True)\n",
    "\n",
    "# Save data to files\n",
    "def write_list_to_file(data_list, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for item in data_list:\n",
    "            f.write(f\"{item}\\n\")\n",
    "\n",
    "write_list_to_file(train_data, os.path.join(flair_data_folder, 'train.txt'))\n",
    "write_list_to_file(dev_data, os.path.join(flair_data_folder, 'dev.txt'))\n",
    "write_list_to_file(test_data, os.path.join(flair_data_folder, 'test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-29 18:15:47,537 Reading data from ../data/flair_data\n",
      "2024-11-29 18:15:47,538 Train: ../data/flair_data/train.txt\n",
      "2024-11-29 18:15:47,538 Dev: ../data/flair_data/dev.txt\n",
      "2024-11-29 18:15:47,538 Test: ../data/flair_data/test.txt\n",
      "2024-11-29 18:15:48,009 Initialized corpus ../data/flair_data (label type name is 'sentiment')\n",
      "Number of training sentences: 22500\n",
      "Number of validation sentences: 2500\n",
      "Number of test sentences: 25000\n"
     ]
    }
   ],
   "source": [
    "# Define the folder where the data is located\n",
    "corpus_folder = Path(flair_data_folder)\n",
    "\n",
    "# Create the corpus\n",
    "corpus = ClassificationCorpus(\n",
    "    corpus_folder,\n",
    "    train_file='train.txt',\n",
    "    dev_file='dev.txt',\n",
    "    test_file='test.txt',\n",
    "    label_type='sentiment'\n",
    ")\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Number of training sentences: {len(corpus.train)}\")\n",
    "print(f\"Number of validation sentences: {len(corpus.dev)}\")\n",
    "print(f\"Number of test sentences: {len(corpus.test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-29 18:15:48,013 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "22500it [00:40, 550.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-29 18:16:28,876 Dictionary created for label 'sentiment' with 2 values: POS (seen 11298 times), NEG (seen 11202 times)\n",
      "Dictionary with 2 tags: POS, NEG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "label_dict = corpus.make_label_dictionary(label_type='sentiment')\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiningonn/anaconda3/envs/nlp_project/lib/python3.8/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Set Up Stacked Embeddings\n",
    "flair_forward_embedding = FlairEmbeddings('news-forward')\n",
    "flair_backward_embedding = FlairEmbeddings('news-backward')\n",
    "transformer_word_embeddings = TransformerWordEmbeddings('distilbert-base-uncased')\n",
    "\n",
    "# List of embeddings\n",
    "embeddings = [\n",
    "    flair_forward_embedding,\n",
    "    flair_backward_embedding,\n",
    "    transformer_word_embeddings,\n",
    "]\n",
    "\n",
    "# Create document embeddings from word embeddings\n",
    "document_embeddings = DocumentRNNEmbeddings(\n",
    "    embeddings=embeddings,\n",
    "    hidden_size=256,\n",
    "    reproject_words=True,\n",
    "    reproject_words_dimension=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = TextClassifier(\n",
    "    document_embeddings,\n",
    "    label_dictionary=label_dict,\n",
    "    label_type='sentiment'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-29 18:16:55,572 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-29 18:16:55,573 Model: \"TextClassifier(\n",
      "  (embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): TransformerWordEmbeddings(\n",
      "        (model): DistilBertModel(\n",
      "          (embeddings): Embeddings(\n",
      "            (word_embeddings): Embedding(30523, 768)\n",
      "            (position_embeddings): Embedding(512, 768)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (transformer): Transformer(\n",
      "            (layer): ModuleList(\n",
      "              (0-5): 6 x TransformerBlock(\n",
      "                (attention): MultiHeadSelfAttention(\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "                )\n",
      "                (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (ffn): FFN(\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (activation): GELUActivation()\n",
      "                )\n",
      "                (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4864, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 256, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=256, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (locked_dropout): LockedDropout(p=0.0)\n",
      "  (word_dropout): WordDropout(p=0.0)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2024-11-29 18:16:55,573 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-29 18:16:55,574 Corpus: 22500 train + 2500 dev + 25000 test sentences\n",
      "2024-11-29 18:16:55,574 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-29 18:16:55,574 Train:  22500 sentences\n",
      "2024-11-29 18:16:55,574         (train_with_dev=False, train_with_test=False)\n",
      "2024-11-29 18:16:55,575 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-29 18:16:55,575 Training Params:\n",
      "2024-11-29 18:16:55,575  - learning_rate: \"5e-05\" \n",
      "2024-11-29 18:16:55,575  - mini_batch_size: \"4\"\n",
      "2024-11-29 18:16:55,576  - max_epochs: \"3\"\n",
      "2024-11-29 18:16:55,576  - shuffle: \"True\"\n",
      "2024-11-29 18:16:55,576 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-29 18:16:55,576 Plugins:\n",
      "2024-11-29 18:16:55,576  - LinearScheduler | warmup_fraction: '0.1'\n",
      "2024-11-29 18:16:55,577 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-29 18:16:55,577 Final evaluation on model after last epoch (final-model.pt)\n",
      "2024-11-29 18:16:55,577  - metric: \"('micro avg', 'f1-score')\"\n",
      "2024-11-29 18:16:55,578 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-29 18:16:55,578 Computation:\n",
      "2024-11-29 18:16:55,578  - compute on device: cpu\n",
      "2024-11-29 18:16:55,578  - embedding storage: none\n",
      "2024-11-29 18:16:55,579 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-29 18:16:55,579 Model training base path: \"flair_model\"\n",
      "2024-11-29 18:16:55,579 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-29 18:16:55,579 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiningonn/anaconda3/envs/nlp_project/lib/python3.8/site-packages/flair/trainers/trainer.py:499: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp and flair.device.type != \"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-29 20:31:42,247 epoch 1 - iter 562/5625 - loss 0.58804855 - time (sec): 8086.67 - samples/sec: 0.28 - lr: 0.000017 - momentum: 0.000000\n",
      "2024-11-29 22:40:59,115 epoch 1 - iter 1124/5625 - loss 0.53251435 - time (sec): 15843.54 - samples/sec: 0.28 - lr: 0.000033 - momentum: 0.000000\n",
      "2024-11-30 00:44:33,174 epoch 1 - iter 1686/5625 - loss 0.52635825 - time (sec): 23257.59 - samples/sec: 0.29 - lr: 0.000050 - momentum: 0.000000\n",
      "2024-11-30 02:46:54,666 epoch 1 - iter 2248/5625 - loss 0.52355535 - time (sec): 30599.09 - samples/sec: 0.29 - lr: 0.000048 - momentum: 0.000000\n",
      "2024-11-30 04:53:32,622 epoch 1 - iter 2810/5625 - loss 0.50514976 - time (sec): 38197.04 - samples/sec: 0.29 - lr: 0.000046 - momentum: 0.000000\n",
      "2024-11-30 06:53:54,399 epoch 1 - iter 3372/5625 - loss 0.49047850 - time (sec): 45418.82 - samples/sec: 0.30 - lr: 0.000044 - momentum: 0.000000\n",
      "2024-11-30 08:52:12,843 epoch 1 - iter 3934/5625 - loss 0.48356723 - time (sec): 52517.26 - samples/sec: 0.30 - lr: 0.000043 - momentum: 0.000000\n",
      "2024-11-30 10:53:54,229 epoch 1 - iter 4496/5625 - loss 0.47445322 - time (sec): 59818.65 - samples/sec: 0.30 - lr: 0.000041 - momentum: 0.000000\n",
      "2024-11-30 12:51:43,554 epoch 1 - iter 5058/5625 - loss 0.46472690 - time (sec): 66887.97 - samples/sec: 0.30 - lr: 0.000039 - momentum: 0.000000\n",
      "2024-11-30 14:51:12,897 epoch 1 - iter 5620/5625 - loss 0.45704510 - time (sec): 74057.32 - samples/sec: 0.30 - lr: 0.000037 - momentum: 0.000000\n",
      "2024-11-30 14:52:11,822 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-30 14:52:11,822 EPOCH 1 done: loss 0.4567 - lr: 0.000037\n",
      "2024-11-30 14:52:11,823 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [58:13<00:00, 22.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-30 15:50:25,834 DEV : loss 0.31884515285491943 - f1-score (micro avg)  0.904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-30 15:50:30,416 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-30 15:53:39,044 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-30 15:53:39,044 Exiting from training early.\n",
      "2024-11-30 15:53:39,045 Saving model ...\n",
      "2024-11-30 15:53:39,510 Done.\n",
      "2024-11-30 15:53:39,511 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-30 15:53:39,511 Testing using last state of model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1563 [00:21<9:21:17, 21.56s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ModelTrainer(classifier, corpus)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#Fine-tune the model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfine_tune\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflair_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# Directory to save the model and logs\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# Learning rate for fine-tuning\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmini_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;66;43;03m# Smaller batch size for transformers\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                        \u001b[49m\u001b[38;5;66;43;03m# Number of epochs\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings_storage_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAdamW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m# Optimizer suited for transformers\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_final_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Save the final model\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_model_each_k_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# Save model checkpoint every epoch\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_file_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Save logs to a file\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_loss_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Save loss values to a file\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_project/lib/python3.8/site-packages/flair/trainers/trainer.py:253\u001b[0m, in \u001b[0;36mModelTrainer.fine_tune\u001b[0;34m(self, base_path, warmup_fraction, learning_rate, decoder_learning_rate, mini_batch_size, eval_batch_size, mini_batch_chunk_size, max_epochs, optimizer, train_with_dev, train_with_test, reduce_transformer_vocab, main_evaluation_metric, monitor_test, monitor_train_sample, use_final_model_for_eval, gold_label_dictionary_for_eval, exclude_labels, sampler, shuffle, shuffle_first_epoch, embeddings_storage_mode, epoch, save_final_model, save_optimizer_state, save_model_each_k_epochs, create_file_logs, create_loss_file, write_weights, use_amp, plugins, attach_default_scheduler, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attach_default_scheduler:\n\u001b[1;32m    251\u001b[0m     plugins\u001b[38;5;241m.\u001b[39mappend(LinearSchedulerPlugin(warmup_fraction\u001b[38;5;241m=\u001b[39mwarmup_fraction))\n\u001b[0;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_custom\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# training parameters\u001b[39;49;00m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_learning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_learning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmini_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmini_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmini_batch_chunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmini_batch_chunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_with_dev\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_with_dev\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_with_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_with_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduce_transformer_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_transformer_vocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# evaluation and monitoring\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmain_evaluation_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_evaluation_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonitor_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor_train_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonitor_train_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_final_model_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_final_model_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgold_label_dictionary_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgold_label_dictionary_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# sampling and shuffling\u001b[39;49;00m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle_first_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle_first_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# evaluation and monitoring\u001b[39;49;00m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings_storage_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings_storage_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# when and what to save\u001b[39;49;00m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_final_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_final_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_optimizer_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_optimizer_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_model_each_k_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_model_each_k_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# logging parameters\u001b[39;49;00m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_file_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_file_logs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_loss_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_loss_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# amp\u001b[39;49;00m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# plugins\u001b[39;49;00m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplugins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplugins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_project/lib/python3.8/site-packages/flair/trainers/trainer.py:785\u001b[0m, in \u001b[0;36mModelTrainer.train_custom\u001b[0;34m(self, base_path, learning_rate, decoder_learning_rate, mini_batch_size, eval_batch_size, mini_batch_chunk_size, max_epochs, optimizer, train_with_dev, train_with_test, max_grad_norm, reduce_transformer_vocab, main_evaluation_metric, monitor_test, monitor_train_sample, use_final_model_for_eval, gold_label_dictionary_for_eval, exclude_labels, sampler, shuffle, shuffle_first_epoch, embeddings_storage_mode, epoch, save_final_model, save_optimizer_state, save_model_each_k_epochs, create_file_logs, create_loss_file, write_weights, use_amp, plugins, **kwargs)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    783\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting using last state of model ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 785\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgold_label_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmini_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest.tsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_storage_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmain_evaluation_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_evaluation_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgold_label_dictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgold_label_dictionary_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    797\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(test_results\u001b[38;5;241m.\u001b[39mdetailed_results)\n\u001b[1;32m    798\u001b[0m log_line(log)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_project/lib/python3.8/site-packages/flair/nn/model.py:297\u001b[0m, in \u001b[0;36mClassifier.evaluate\u001b[0;34m(self, data_points, gold_label_type, out_path, embedding_storage_mode, mini_batch_size, main_evaluation_metric, exclude_labels, gold_label_dictionary, return_loss, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m     datapoint\u001b[38;5;241m.\u001b[39mremove_labels(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# predict for batch\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m loss_and_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_storage_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_storage_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmini_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmini_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredicted\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_loss:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loss_and_count, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_project/lib/python3.8/site-packages/flair/nn/model.py:857\u001b[0m, in \u001b[0;36mDefaultClassifier.predict\u001b[0;34m(self, sentences, mini_batch_size, return_probabilities_for_all_classes, verbose, label_name, return_loss, embedding_storage_mode)\u001b[0m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;66;03m# pass data points through network and decode\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m data_point_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_data_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    858\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(data_point_tensor)\n\u001b[1;32m    859\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mask_scores(scores, data_points)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_project/lib/python3.8/site-packages/flair/nn/model.py:726\u001b[0m, in \u001b[0;36mDefaultClassifier._encode_data_points\u001b[0;34m(self, sentences, data_points)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_data_points\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentences: List[DT], data_points: List[DT2]):\n\u001b[1;32m    724\u001b[0m     \u001b[38;5;66;03m# embed sentences\u001b[39;00m\n\u001b[1;32m    725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_embed_sentence:\n\u001b[0;32m--> 726\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;66;03m# get a tensor of data points\u001b[39;00m\n\u001b[1;32m    729\u001b[0m     data_point_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_embedding_for_data_point(data_point) \u001b[38;5;28;01mfor\u001b[39;00m data_point \u001b[38;5;129;01min\u001b[39;00m data_points])\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_project/lib/python3.8/site-packages/flair/embeddings/base.py:50\u001b[0m, in \u001b[0;36mEmbeddings.embed\u001b[0;34m(self, data_points)\u001b[0m\n\u001b[1;32m     47\u001b[0m     data_points \u001b[38;5;241m=\u001b[39m [data_points]\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_everything_embedded(data_points):\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_embeddings_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_points\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_project/lib/python3.8/site-packages/flair/embeddings/document.py:333\u001b[0m, in \u001b[0;36mDocumentRNNEmbeddings._add_embeddings_internal\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# embed words in the sentence\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m lengths: List[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(sentence\u001b[38;5;241m.\u001b[39mtokens) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences]\n\u001b[1;32m    336\u001b[0m longest_token_sequence_in_batch: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(lengths)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_project/lib/python3.8/site-packages/flair/embeddings/token.py:97\u001b[0m, in \u001b[0;36mStackedEmbeddings.embed\u001b[0;34m(self, sentences, static_embeddings)\u001b[0m\n\u001b[1;32m     94\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [sentences]\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings:\n\u001b[0;32m---> 97\u001b[0m     \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_project/lib/python3.8/site-packages/flair/embeddings/base.py:50\u001b[0m, in \u001b[0;36mEmbeddings.embed\u001b[0;34m(self, data_points)\u001b[0m\n\u001b[1;32m     47\u001b[0m     data_points \u001b[38;5;241m=\u001b[39m [data_points]\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_everything_embedded(data_points):\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_embeddings_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_points\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_project/lib/python3.8/site-packages/flair/embeddings/token.py:815\u001b[0m, in \u001b[0;36mFlairEmbeddings._add_embeddings_internal\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    812\u001b[0m end_marker \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;66;03m# get hidden states from language model\u001b[39;00m\n\u001b[0;32m--> 815\u001b[0m all_hidden_states_in_lm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_representation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_marker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_marker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchars_per_chunk\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfine_tune:\n\u001b[1;32m    820\u001b[0m     all_hidden_states_in_lm \u001b[38;5;241m=\u001b[39m all_hidden_states_in_lm\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_project/lib/python3.8/site-packages/flair/models/language_model.py:160\u001b[0m, in \u001b[0;36mLanguageModel.get_representation\u001b[0;34m(self, strings, start_marker, end_marker, chars_per_chunk)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[1;32m    159\u001b[0m     batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 160\u001b[0m     rnn_output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     output_parts\u001b[38;5;241m.\u001b[39mappend(rnn_output)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# concatenate all chunks to make final output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_project/lib/python3.8/site-packages/flair/models/language_model.py:88\u001b[0m, in \u001b[0;36mLanguageModel.forward\u001b[0;34m(self, input, hidden, ordered_sequence_lengths, decode)\u001b[0m\n\u001b[1;32m     86\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m (h,)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(output)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_project/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_project/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_project/lib/python3.8/site-packages/torch/nn/modules/rnn.py:917\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    914\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    921\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "import logging\n",
    "\n",
    "# Set the logging level to INFO\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "#Fine-tune the model\n",
    "trainer.fine_tune(\n",
    "    base_path='flair_model',             # Directory to save the model and logs\n",
    "    learning_rate=5e-5,                  # Learning rate for fine-tuning\n",
    "    mini_batch_size=4,                   # Smaller batch size for transformers\n",
    "    max_epochs=3,                        # Number of epochs\n",
    "    embeddings_storage_mode='none',     \n",
    "    optimizer=AdamW,                     # Optimizer suited for transformers\n",
    "    save_final_model=True,               # Save the final model\n",
    "    save_model_each_k_epochs=1,          # Save model checkpoint every epoch\n",
    "    create_file_logs=True,               # Save logs to a file\n",
    "    create_loss_file=True,               # Save loss values to a file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model_path = \"flair_model/best_model.pt\"  # Adjust this to the path of your saved model\n",
    "classifier = TextClassifier.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(classifier, test_dataset):\n",
    "    \"\"\"\n",
    "    Evaluate a Flair classifier on a given test dataset.\n",
    "    Args:\n",
    "        classifier (TextClassifier): The trained Flair classifier.\n",
    "        test_dataset (Dataset): The test dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    # Iterate over test dataset with tqdm progress bar\n",
    "    for sentence in tqdm(test_dataset, desc=\"Evaluating\", leave=True):\n",
    "        # True label\n",
    "        true_labels.append(sentence.get_label(\"sentiment\").value)\n",
    "\n",
    "        # Predicted label\n",
    "        classifier.predict(sentence)\n",
    "        predicted_labels.append(sentence.labels[0].value)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, pos_label=\"POS\")\n",
    "    recall = recall_score(true_labels, predicted_labels, pos_label=\"POS\")\n",
    "    f1 = f1_score(true_labels, predicted_labels, pos_label=\"POS\")\n",
    "\n",
    "    # Full classification report\n",
    "    classification_rep = classification_report(true_labels, predicted_labels, target_names=[\"POS\", \"NEG\"])\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_rep)\n",
    "\n",
    "    # Return metrics as a dictionary\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"classification_report\": classification_rep,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flair'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflair\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClassificationCorpus\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the corpus (adjust the path to your dataset)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m corpus \u001b[38;5;241m=\u001b[39m ClassificationCorpus(flair_data_folder, test_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, label_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flair'"
     ]
    }
   ],
   "source": [
    "from flair.datasets import ClassificationCorpus\n",
    "\n",
    "# Load the corpus (adjust the path to your dataset)\n",
    "corpus = ClassificationCorpus(flair_data_folder, test_file=\"test.txt\", label_type=\"sentiment\")\n",
    "\n",
    "# Access the test sentences\n",
    "test_sentences = corpus.test\n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_model(classifier, small_corpus.test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
